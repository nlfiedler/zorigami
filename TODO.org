* Tasks
** Replace Time Machine
*** DONE Support wildcard exclusions (e.g. =*/node_modules/**=)
*** DONE Update GraphQL to support returning and changing the excludes
*** DONE Provide GUI for managing multiple exclusions for each data set
*** how to update local deployment
**** installed in =~/Applications/Zorigami=
**** launch agent in =~/Library/LaunchAgents/zorigami.plist=
**** database in =~/Library/Application\ Support/Zorigami=
**** stop server: =launchctl kill SIGTERM gui/501/zorigami=
**** enable, start, and verify server is running
#+begin_src shell
launchctl enable gui/501/zorigami
launchctl kickstart -p gui/501/zorigami
ps -ef | grep -i zorigami
#+end_src
*** TODO develop as a macOS app that bundles the =server= and starts at login
*** TODO Use =launchd= to manage the process, have it start automatically
**** launchd definition
#+begin_src xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
    <dict>
        <key>Label</key>
        <string>zorigami</string>
        <key>Program</key>
        <string>/Users/nfiedler/Applications/Zorigami/server</string>
        <key>WorkingDirectory</key>
        <string>/Users/nfiedler/Applications/Zorigami</string>
        <key>RunAtLoad</key>
        <true/>
        <key>EnvironmentVariables</key>
        <dict>
            <key>DB_PATH</key>
            <string>/Users/nfiedler/Library/Application Support/Zorigami/dbase</string>
            <key>HOST</key>
            <string>0.0.0.0</string>
            <key>PORT</key>
            <string>8000</string>
            <key>RUST_LOG</key>
            <string>info</string>
        </dict>
        <key>StandardErrorPath</key>
        <string>/Users/nfiedler/Library/Application Support/Zorigami/error.log</string>
        <key>StandardOutPath</key>
        <string>/Users/nfiedler/Library/Application Support/Zorigami/output.log</string>
    </dict>
</plist>
#+end_src
**** how to start
#+begin_src shell
launchctl load ~/Library/LaunchAgents/zorigami.plist
#+end_src
*** TODO figure out how to give it permission to access files on macOS
*** DONE pack prune tries to remove OS folders, results in IO error
*** DONE need a query that shows the counts for all record types
**** DONE write integration for =count_prefix()= in database impl
**** DONE write usecase and unit tests
**** DONE write graphql query field and tests
*** TODO consider which records could be shrunk to save disk space
**** compressed database backup image size in bytes: 2,379,181,138
**** record counts on mini as of 2022-03-15
| entity    |   count |
|-----------+---------|
| snapshots |     190 |
| files     | 1806620 |
| trees     |  302860 |
| chunks    | 1830167 |
| packs     |    3380 |
| xattrs    |  185473 |
**** files and chunks are the most numerous types of records
**** TODO do not serialize the =length= field of chunk model
**** TODO do not create a chunk record for small files
***** strategy: if length of =File.chunks= list is 1 then it is a pack instead of a chunk
***** when checking if a chunk has already been processed, need to consider small files as well
***** in =record_completed_files()= if the =parts.len()= is 1 then handle differently
****** store pack digest instead of the chunk checksum in the =File= entity
***** file restore needs to consider when list of chunks is length 1
*** TODO need a query to show details about the latest snapshot
**** show number of files
**** show number of trees
*** TODO need a way to show differences between any two snapshots
**** collect the paths and sizes of all new/changed files
**** somehow show all of that information in a scalable fashion
*** TODO change default pack size to 64mb instead of 16
*** TODO database backup procedure should prune old snapshots
**** for local disk case, retain only a few snapshots
**** for cloud case, honor the least expensive retention plan
** Restore to dissimilar hardware
*** TODO Allow setting the configuration to change the identity
*** TODO Test by restoring a backup to a different system
** Manual Backup controls
*** write use cases for starting and stopping a backup
*** add a "backup now" button to datasets listing
**** need a GraphQL mutation to signal backup to start
**** add a =start_dataset_now()= in =supervisor= module, similar to =start_due_datasets()=
***** that is, enqueue =StartBackup= on the =Runner= actor
*** similiarly have a "stop backup" button if it is running
**** need a GraphQL mutation to signal backup to stop
**** add a =StopBackup= action in =state= module
**** the =StopBackup= action sets =stop_requested= in =BackupState=
**** then =handle_file()= in =engine= module calls =get_state()= and checks for =stop_requested=
**** =handle_file()= will return an error if =stop_requested= is true
*** consider how one might "pause" a backup in progress
** Improved interface
*** c.f. https://duplicacy.com/guide.html webui looks good
** Loose backend issues
*** TODO consider replacing =DBPath= in tests with =tempfile=
**** using =DBPath= and async code and =Path::exists()= causes a =SIGILL=
*** TODO testing the minio(?) pack store showed a tokio runtime error
*** TODO refine use of =&str= and =String= arguments by using =Into<String>=
**** note that using =Cow= helps to minimize copying
#+BEGIN_SRC rust
pub fn name<T: Into<String>>(mut self, name: T) -> Self {
    self.name = Cow::Owned(name.into());
    self
}
#+END_SRC
*** TODO Too many open files (in RocksDB)
**** need to set =set_max_open_files()= on database options
**** default ulimit on macOS is 256, so something less would be ideal
**** ran out of files in tanuki when rocksdb directory contained 217 files
**** maybe consider a means of countering this error at runtime
*** TODO the monthly fuzzy schedule test fails on the 30th of the month
** Loose GraphQL tasks
*** TODO fix the GraphQL query/mutation names to be snakecase
*** TODO schema custom types need some unit tests
**** especially the schedule validation code
*** TODO probably should use a better client cache
**** c.f. =graphql_flutter= example that implements a =uuidFromObject()= function
**** uses the "type" of the object and its unique identifier as the caching key
**** our objects would need to have a "typename" for this to work
*** TODO find out how to document arguments to mutations
**** c.f. juniper API docs: Attribute Macro juniper::object
** Loose WebUI tasks
*** TODO pack store ~test~ feature shows snackbar repeatedly
*** TODO sometimes get an HTTP error in GraphQL client
**** should automatically retry the query a few times before giving up
*** TODO test with a smaller browser window to surface sizing issues
*** TODO when there are no snapshots, clicking the dataset row does nothing
*** TODO how to refresh the snapshots screen?
**** gets stale as soon as a backup has been run
**** navigation to the snapshots does not work if there were none to start with
**** maybe add a refresh button like in google cloud console
*** TODO schedule start/stop times should be using local time (no excuse for not doing this)
*** TODO local store basepath and google credentials should use file picker
**** https://pub.dev/packages/form_builder_file_picker
*** TODO improve the navigation drawer
**** currently selected option should be highlighted, not actionable
*** TODO improve (server) error handling
**** when a temporary server error occurs, offer a "Retry" button
*** TODO improve snapshot tree browser
**** should sort entries by filename case-insensitively
**** for larger number of entries, should use =PaginatedDataTable=
**** nice to have: sticky table header
**** nice to have: sort by file type
*** TODO consider how to hide the minio secret key using a show/hide button
*** TODO consider approaches to l10n and i18n
**** c.f. https://resocoder.com/2019/06/01/flutter-localization-the-easy-way-internationalization-with-json/
*** TODO improve the data sets form
**** TODO FAB covers the =DELETE= button even when scrolled all the way down
**** TODO use the =validate()= function on =DataSet= to ensure validity
**** TODO should decode the computer ID to improve readability
*** TODO should sort the datasets so they are always in the same order
**** maybe sort them by date, with most recent first
*** TODO tree entries of =ERROR= type should be displayed as such
**** error message from =TreeEntry.new()= could be stored as a new type of =TreeReference=
***** e.g. =TreeReference.ERROR(String)= where the string is the error message
*** TODO should have ui for listing all snapshots in a dataset
**** consider presenting in a style similar to Time Machine
**** e.g. a timeline of the snapshots
**** c.f. https://pub.dev/packages/flutter_timeline
**** probably need paging in the ui and graphql api
*** TODO improve the page for defining stores
**** TODO delete button should be far away from the other button(s)
**** TODO delete button should require two clicks, with "are you sure?"
*** TODO use breadcrumbs in the tree navigator to get back to parent directories
*** TODO consider and improve accessibility
**** enable testing for a11y sanity
**** add hints to improve the presentation of information
***** configuration panel
***** snapshot browser
** Improved error handling
*** Pack store input validation
**** should validate Google Cloud service account key when defining pack
*** Consider a structured design for error types and handling
**** c.f. https://fettblog.eu/rust-enums-wrapping-errors/
*** Look at https://github.com/dtolnay/thiserror for defining error types
*** May improve error handling and reporting
** Initial Configuration
*** Walk user through pack store and data set creation
*** Offer path for restoring database from existing pack store
*** Allow user to set user/host names for computer UUID
**** They may need to avoid naming conflicts with other local users
**** Imagine a computer lab all sharing a single cloud storage account
** Remote pack store interaction
*** Remote pack stores like Google Cloud have built-in limits for certain operations
**** need to consider that GCS will limit the number of buckets listed to 1,000
**** probably minio and/or S3 have similar default limits
**** the API generally offers a means of paging to get everything in chunks
** Remove files/folders from backup
*** Allow removing files from existing backups
**** e.g. accidentally saved large binaries
** Snapshot Pruning
*** Use a multi-phased approach with pruning and garbage collection
*** Must not run collection while a backup is in progress
*** Must prevent a backup from starting while pruning is in progress
*** Phase 1: prune snapshots based on a policy
**** set the child's parent reference to skip over stale snapshot
**** e.g. remove snapshots more then N days old
**** e.g. keep N snapshots per day, M per week, and P per month
*** Phase 2: prune unreachable objects in the database
**** copy everything reachable to a new database instance
****** datasets -> snapshots -> trees -> files -> chunks -> packs
**** delete the old database
*** Phase 3: prune unreferenced packs from pack store
**** honor cloud data retention policies
***** e.g. typically anything older than 90 days costs nothing to delete
***** Google has different minimum storage durations for each storage class
****** https://cloud.google.com/storage/docs/storage-classes
***** user can specify their own value for each pack store if necessary
*** Phase 4: prune old database snapshots (no need to keep old copies)
**** honor cloud data retention policies
**** use the =upload_time= in the =Pack= record to determine age
*** Implementation should follow Clean Architecture to improve testability
**** entities and use case separated from data sources via repositories
**** this allows for easily mocking up data to feed the pruning use case
***** i.e. when the use case asks for trees and such, give it mock data structures
** Advanced Scheduling
*** backend
**** Permit ~hourly~ backups every N hours
**** Permit ~daily~ backups every N days
**** Permit ~weekly~ backups every N weeks
**** Permit ~monthly~ backups every N months
*** frontend
**** TODO Support multiple schedules in interface
**** TODO Support day-of-week in schedule
**** TODO Support day-of-month in schedule
**** TODO Support week-of-month in schedule
**** TODO Support time-range in schedule
** More Functionality
*** TODO search snapshots to find a file/directory by a given pattern
**** the file/dir is not in the latest snapshot but some older one, go find it
**** might not even know the full path of the file/dir in question
*** TODO store restore requests in database to tolerate application restart
**** currently restore requests are queued in memory only, so a crash means everything is forgotten
*** TODO Perform a full backup on demand, discard all previous backups
**** Wifey doesn't like the idea of accumulating old stuff
**** Gives the user a chance to save space by removing old content
**** Optionally prune all existing packs in the process
*** TODO event dispatching for the web and desktop
**** use the state management to manage "events" and state
**** engine emits actions/events to the store
***** for backup and restore functions
***** e.g. "downloaded a pack", "uploaded a pack"
**** store holds the cumulative data so late attachers can gather everything
**** supervisor threads register as subscribers to the store
**** clients will use GraphQL subscriptions to receive updates
**** supervisor threads emit GraphQL subscription events
*** TODO consider how datasets can be modified after creation
**** cannot change stores assigned to dataset once there are snapshots
**** basically would require starting over if changing stores, base path, etc
*** TODO consider how to restore symbolic links
**** i.e. no file chooser to download anything
**** what if the same path is now a file/directory?
*** TODO Secure FTP improvements
**** TODO support SFTP with private key authentication
***** use store form to take paths for public and private keys
**** TODO allow private key that is locked with a passphrase
***** passphrase for private key would be provided by envar
*** TODO Repair missing pack files in pack stores
**** expose the GraphQL operation via the graphical interface
** More Information
*** TODO Show details about snapshots and files
**** show differences between two snapshots
**** show pack/chunk metrics for   all   files in a snapshot
**** show pack/chunk metrics for changed files in a snapshot
*** TODO Query to see histogram of file sizes, number of chunks, etc
**** for a given snapshot
***** count number of files with N chunks for all values of N
*** TODO Show number of packs stored in a pack store
** Database Integrity
*** support database integrity checks
**** ensure all referenced records actually exist
**** like git fsck, start at the top and traverse everything
**** find and report dangling objects
**** an automated scan could be run on occasion
** Pack file integrity
*** Retrieve random pack files and verify integrity
** Architecture Review
*** Database per dataset directory
**** Centralized configuration in a known location
***** would default to something sensible in user home directory
***** overridden by environment variable
***** JSON or XML formatted plain text file
***** Holds paths to the various data sets
***** Holds pack store configuration
**** Each data set directory has a database directory (and backup)
**** Backup process automatically excludes the database and its backup
**** What would a full restore procedure look like?
**** Benefits
***** reduced risk in the event of database corruption
**** Drawbacks
***** additional disk usage for database overhead
***** forces user to keep database with the dataset
*** Parallel backups
**** Currently the backup supervisor spawns a single thread (=Arbiter=) to manage backups
**** This causes all backups to be serialized
**** For parallel backups, would use the =SyncArbiter= from actix
*** Database migrations
**** Use the =serde= crate features (c.f. https://serde.rs)
**** Use =#[serde(default)]= on struct to fill in blanks for new fields
**** Add =#[serde(skip_serializing)]= to a deprecated struct field
**** New fields will need accessors that convert from old fields as needed
***** reset the old field to indicate it is no longer relevant
**** Removing a field is no problem for serde
*** Shared pack repository
**** Current design basically forces each user/install to have a separate pack repo
**** Otherwise the pack pruning would delete the packs for other users saving to the same repo
*** Embedded Database
**** Is the default RocksDB performance sufficient?
**** Consider https://github.com/spacejam/sled/
***** written in Rust, open source
***** will need prefix key scanning
****** looks like you just use a prefix of the key (sorts before the matching keys)
***** will need backup/restore functions
*** Client/Server
**** Look at ways to secure the server, to allay fears of exploits
**** A web conferencing tool was exploited via its hidden HTTP server
** Desktop application
*** design a configuration system for desktop
**** define the whole clean architecture setup
***** entities, use cases, repositories
**** data source for web will have values defined by environment_config only
**** data source for desktop will use shared preferences (?) for persistence
**** data layer repository chooses between data sources based on environment
***** how to detect if application was compiled for web
#+BEGIN_SRC dart
import 'package:flutter/foundation.dart' show kIsWeb;
if (kIsWeb) { /* web stuff */ } else { /* not web */ }
#+END_SRC
*** clipboard support
**** look for clipboard plugin for flutter (for macOS)
**** c.f. https://flutter.dev/docs/development/packages-and-plugins/developing-packages
** macOS support
*** TODO optional Time Machine style backup and retention policy
**** hourly backups for 24 hours
**** daily backups for 30 days
**** weekly backups for everything else
**** prune backups to maintain a certain size
** Full Restore
*** Procedure for full restore
**** User installs and configures application
**** User invokes "full restore" function
**** User provides a temporary pack store configuration
**** Query pack store to get candidate computer UUID values
**** User chooses database to restore
***** if current UUID matches one in the available set, select it by default
**** Fetch the most recent database files
***** Restore to a different directory, then copy over records
***** Copy every record except for =configuration= (and maybe others?)
***** Copy records for datasets, stores, snapshots, packs, etc
**** User can now browse datasets and restore as usual
**** Restoring an entire dataset is simply the "tree restore" case
*** Walk the user through the process
**** Configure the primary pack store for retrieval
**** Inform user that this pack store configuration is only temporary
**** Select database to retrieve based on computer UUID
**** Instruct user to restore as usual from dataset(s)
*** TODO Restore symbolic links (currently does nothing with =TreeReference::LINK=)
*** TODO Restore file attributes from tree entry
**** TODO File mode
**** TODO File user/group
**** TODO File extended attributes
*** TODO Restore directories from snapshot
**** TODO Directory mode
**** TODO Directory user/group
**** TODO Directory extended attributes
**** TODO Restore multiple files efficiently
**** TODO Restore a directory tree efficiently
*** TODO Detect and prune stale snapshots that never completely uploaded
**** Stale snapshots exist in the database but are not referenced elsewhere
*** TODO Support snapshots consisting only of mode/owner changes
**** i.e. no file content changes, just the database records
** Windows support
*** TODO Does Microsoft's ~Windows Package Manager~ (=winget=) provide an easier dev setup path?
*** TODO Backup files opened by a running process
**** Normally cannot read files that are opened on Windows
**** See Volume Shadow Copy Services (VSS) for details
*** TODO Support Windows file types
**** ReadOnly
**** Hidden
**** System
** More Better
*** TODO Permit scheduling upload hours for each day of the week
**** e.g. from 11pm to 6am Mon-Fri, none on Sat/Sun
*** TODO Command-line option to dump database to json (separate by key prefix, e.g. ~chunk~)
*** TODO Support deduplication across multiple computers
**** Place the chunks and packs in a seperate "database" for syncing
***** For RocksDB, use a column family if it helps with =GetUpdatesSince()=
**** RocksDB replication story as of 2019-02-20:
: Q: Does RocksDB support replication?
: A: No, RocksDB does not directly support replication. However, it offers
: some APIs that can be used as building blocks to support replication.
: For instance, GetUpdatesSince() allows developers to iterate though all
: updates since a specific point in time.
***** see =GetUpdatesSince()= and =PutLogData()= functions
**** User configures the host name of the ~peer~ installation
***** Use that to form the URL with which to =sync=
**** Share the chunks and packs documents with a ~peer~ installation
**** At the start of backup, sync with the ~peer~ to get latest chunks/packs
*** TODO Consider how to deal with partial uploads
**** e.g. Minio/S3 has a means of handling these
*** TODO Permit removing a store from a dataset
**** would encourage user to clean up the remote files
**** for local store, could remove the files immediately
**** must invalidate all of the snapshots effected by the missing store
*** TODO Permit moving from one store to another
**** would mean downloading the packs and uploading them to the new store
*** TODO Support Amazon S3
**** Minio seems to have no bucket limit (higher than 100)
**** Need to limit number of remote buckets to 100
**** Bucket limit: catch the error and handle by re-using another bucket
*** TODO Support Amazon Glacier
**** Need to limit number of remote buckets to 1000
**** Use S3 to store the database-to-archive mapping of each snapshot
**** Offer user option to use "expedited" retrievals so they go faster
*** TODO Support Amazon Cloud Drive
*** TODO Support Microsoft Azure blob storage
*** TODO Support Backblaze B2
*** TODO Support [[https://wiki.openstack.org/wiki/Swift][OpenStack Swift]]
*** TODO Support Wasabi
*** TODO Support Google Drive
*** TODO Support Google Cloud Coldline
*** TODO Support Dropbox
*** TODO Support Oracle Cloud Storage
*** TODO Support IBM Cloud Storage
*** TODO Support Rackspace Cloud Files
*** TODO Consider how to backup and restore FIFO, BLK, and CHR "files"
**** c.f. https://github.com/jborg/attic/blob/master/attic/archive.py
**** c.f. https://github.com/avz/node-mkfifo (for FIFO)
**** c.f. https://github.com/mafintosh/mknod (for BLK and CHR)
* Product
** Why another backup program?
*** Original reason in 2014: nothing supported Linux and Glacier
*** What will set this apart from other offerings?
**** Easy to use graphical interface (both desktop and web)
**** Cross-platform (Windows, macOS, Linux)
**** Support for multiple backends, including Glacier
** TODO Define an MVP and work toward release
** TODO Evaluate other backup software
*** TODO Check out some on App Store
**** Backup Guru LE
**** ChronoSync Express
**** Backup
**** Remote Backup Magic
**** Sync - Backup and Restore
**** Backup for Dropbox
**** Freeze - for Amazon Glacier
*** Lot of "folder sync" apps out there
** TODO Define the target audience
*** Average home user, no technical expertise required
** TODO Need distinquishing features
*** What sets this application apart from the other polished products?
**** Cross-platform (e.g. macOS, Windows)
**** Linux server ready
** Application Monitoring
*** Need something to capture failures for debugging
**** c.f. https://sentry.io/welcome/
** Windows Certified
*** CloudBerry(?) has bunches of certifications
*** is that really so meaningful? *I* never cared
** Alternatives
*** Commercial
**** Arq
***** https://www.arqbackup.com
***** Windows, Mac
***** Uses a single master password
***** Supports numerous backends
**** Carbonite
***** https://www.carbonite.com
***** Consumer and business
***** Billed monthly
***** 128-bit encryption for all but most expensive plan
***** Windows, Mac, and "servers"
***** Seems to backup to their servers
**** CloudBerry
***** https://www.cloudberrylab.com/backup
***** Consumer and business
***** Windows, Mac, Linux
***** Supports Glacier and other services
***** Freeware version lacks compression, encryption, limited to 200GB
**** Duplicacy
***** https://github.com/gilbertchen/duplicacy
***** Lists other open source tools and compares them
***** Deduplicates chunks across systems
***** Does not use a database supposedly
***** Does not and can not support Glacier
**** JungleDisk
***** https://www.jungledisk.com/encrypted-backups/
***** Primarily business oriented
***** Seems to rely on their servers
***** Probably stores data elsewhere
**** Rebel Backup
***** https://www.svsware.com/rebelbackup
***** Encrypted backups to Dropbox or Google Drive
***** macOS only
**** qBackup
***** https://www.qualeed.com/en/qbackup/
***** Windows, Mac, Linux
***** Supports numerous backends
***** Has copious documentation with screen shots
**** tarsnap
***** https://www.tarsnap.com
***** Free client
***** Uses public key encryption rather than a password
***** Stores data in Amazon S3
***** Relies on tarsnap servers
***** 10x the price of Google Cloud or Amazon Glacier
***** Command-line interface
*** Open Source
**** Attic
***** https://attic-backup.org
***** Development stopped in 2015
***** Only supports SSH remote host
***** Command-line interface
**** Borg
***** https://borgbackup.readthedocs.io/en/stable/
***** Fork of Attic
***** Only supports SSH remote host
***** Command-line interface
**** bup
***** https://bup.github.io
***** Git-like (uses Python and Git) pack file storage
***** Requires a bup server for remote storage
***** Command-line interface
**** Duplicati
***** https://www.duplicati.com/
***** Requires .NET or Mono
***** Web-based interface
***** Windows and Linux
***** No Glacier support
**** duplicity
***** http://duplicity.nongnu.org
***** Uses GnuPG, a tar-like format, and rsync
***** Supports backends with a filesystem-like interface
***** Command-line interface
***** No Glacier support
**** rclone
***** https://github.com/rclone/rclone
***** Syncs a directory structure to the cloud
***** Offers chunking and encryption
***** Command-line interface
***** No Glacier support
**** restic
***** https://restic.net
***** Git-like data model
***** Supports numerous backends (no Glacier)
***** Command-line interface
** Name
*** Joseph suggests "Attic"
**** =atticapp.com= is taken
**** =attic.app= is for sale
**** Look for ~attic~ in different languages
**** Esperanto: ~mansardo~
***** also means something in Macedonian
**** Hawaiian: ~kaukau~
**** Latin: ~atticae~
* Documentation
** Duplicati has a fun description of how the backup works
*** files are broken into "bricks" which go in "bags" and stored in big "boxes" (the pack store)
*** c.f. https://duplicati.readthedocs.io/en/latest/01-introduction/
** TODO Third party license attributions
*** Include any/all third party license attribution somewhere
*** =cargo lichking bundle= will dump everything to the console
** TODO document how the user might change the passphrase over time
*** user must remember their old passwords in order to decrypt old pack files
*** the application will never store the actual password anywhere
*** will need to prompt the user when a different passphrase is needed
* Technical Information
** Data Growth
*** Database backup tgz seems to grow 8mb in 6 months
** Possible corner cases
*** Database backup, then restore, then pack prune
Because the database snapshot is recorded in the database after the snapshot
has already been uploaded, if the user were to restore the database and then
perform a pack pruning, the most recent database snapshot would be removed.
** JS Build Artifacts
*** Flutter => main.dart.js
| State      |    Size |
|------------+---------|
| production | 1742125 |
*** ReasonML + Webpack => main.js
| State       |    Size |
|-------------+---------|
| development | 2761882 |
| production  |  536345 |
| gzipped     |  145785 |
** Exploring other languages
*** Compile to native for easy deployment
*** Compile to native for code obfuscation
*** Rust
**** Advantages
***** compile to native
***** expressive, safe type system
***** good dependency management
***** lots of useful tools (e.g. clippy)
**** Disadvantages
***** fewer libraries compared to Go
*** Go vs Rust
**** Go: first class support for cloud services
**** Go: statically linked OpenPGP readily available
**** Go: easy to read and write language
**** Rust: mature dependency management tooling
**** Rust: cargo has good editor support
**** Rust: expressive type system
**** Rust: nominal subtyping is much easier to follow
**** Rust: streamlined error handling
**** Rust: fine-grained namespaces and visibility control
