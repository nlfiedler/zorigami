* Tasks
** TODO collect before and after metrics for comparison
*** =test_pack_builder_single= unit test
| when   | size |
|--------+------|
| before | 6656 |
| after  | 3572 |
*** grep the log output for =complete for= to get ~packs~ and ~files~
*** before (using gzip compressed chunks in tar)
**** backup complete after 40 minutes (local and S3)
| system | packs |  files | files/pack |
|--------+-------+--------+------------|
| mini   |    82 | 335675 |       4093 |
| kohaku |  3974 | 132682 |         33 |
*** after (using 7z to compress and archive)
**** backup complete after 1 hours 6 minutes (local only)
**** average pack size 68,148,273
| system | packs |  files | files/pack |
|--------+-------+--------+------------|
| mini   |    67 | 332289 |       4959 |
| kohaku |       |        |            |
** TODO Use version ~0.2.4~ of =sevenz=
** TODO Test database restore from Amazon
*** need to use a new collection, old one has gzipped database snapshots
** TODO Test file restore from Amazon
*** need to use a new collection, old one has tar pack files
** DONE Test file restore from Azure
** TODO Set up mini backup to use Azure (=zorigamini=) and local
** Concurrent pack file creation
*** consider how to produce pack files concurrently
*** mac mini is keeping a single CPU core busy at 95%, while reading ~2mb/sec
** Dynamic chunk size calculation
*** maybe calculate chunk size based on file size distribution
**** already collecting the data during the snapshot phase
**** if there are many large files, then chunk sizes should be larger
**** chunk sizes less than 1kb would be too much overhead to track
**** chunk size minimum of 32kb for typical workloads, 64kb would be safer
**** for backups, 1mb to 4mb chunk sizes is probably reasonable
**** should store the computed chunk size in the database
*** Main server stats as of 2023-01-14
**** compressed database snapshot: 41,109,539
**** Record counts
| entity    |  count |
|-----------+--------|
| snapshots |    272 |
| files     | 134950 |
| trees     |  65312 |
| chunks    |  56813 |
| packs     |   4107 |
| xattrs    |  18035 |
**** File size distribution
| Less than Size | Count |
|----------------+-------|
|             80 |  1418 |
|           1024 | 14997 |
|          10240 | 34084 |
|         102400 | 42247 |
|        1048576 | 23365 |
|       10485760 | 31305 |
|      104857600 |  1209 |
|       infinity |   585 |
**** File sizes by powers of two
***** majority of files are less than 4mb
#+begin_src shell
$ find . -type f -print0 | xargs -0 ls -l | awk '{size[int(log($5)/log(2))]++}END{for (i in size) printf("%11d %5d\n", 2^i, size[i])}' | sort -n
          0   121
          1     2
          8    11
         16    74
         32   963
         64  1492
        128  2956
        256  2184
        512  8730
       1024 10525
       2048 21755
       4096  1296
       8192  3071
      16384  7231
      32768 25297
      65536  8580
     131072  7497
     262144  7061
     524288  7401
    1048576 18137
    2097152 12139
    4194304   949
    8388608   242
   16777216   328
   33554432   443
   67108864   402
  134217728   225
  268435456   166
  536870912    55
 1073741824     7
 2147483648     3
 4294967296     2
34359738368     1
#+end_src
*** Mac mini stats as of 2023-01-14
**** compressed database snapshot: 791,955,794
**** Record counts
| entity    |  count |
|-----------+--------|
| snapshots |    279 |
| files     | 740122 |
| trees     | 126189 |
| chunks    |   6687 |
| packs     |    470 |
| xattrs    |   4842 |
**** File size distribution
| Less than size |  Count |
|----------------+--------|
|             80 |  55321 |
|           1024 | 160793 |
|          10240 | 350121 |
|         102400 |  78929 |
|        1048576 |   7907 |
|       10485760 |    848 |
|      104857600 |    201 |
|       infinity |      9 |
**** File sizes by powers of two
***** 654,129 saved files versus 607,152 counted files
***** half of all files (305,012) are between 1kb and 4kb
***** nearly all files (559,676) are 8kb or less
#+begin_src shell
$ find . -name .Trash -prune -o -name Library -prune -o -name Downloads -prune -o -name node_modules -prune -o -name target -prune -o -type f -print0 | xargs -0 ls -l | awk '{size[int(log($5)/log(2))]++}END{for (i in size) printf("%10d %6d\n", 2^i, size[i])}' | sort -n
         0   1164
         1     92
         2   2156
         4    163
         8    421
        16  44117
        32   4415
        64   7535
       128  12256
       256  53875
       512  74758
      1024 100676
      2048  93775
      4096 110561
      8192  53712
     16384  22738
     32768  12834
     65536   6279
    131072   3149
    262144   1010
    524288    512
   1048576    373
   2097152    169
   4194304    152
   8388608    165
  16777216     39
  33554432     39
  67108864      8
 134217728      7
 268435456      2
#+end_src
** Replace Time Machine
*** TODO develop as a macOS app that bundles the =server= and starts at login
**** consider if =launchd= is needed for this
**** launchd definition
#+begin_src xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
    <dict>
        <key>Label</key>
        <string>zorigami</string>
        <key>Program</key>
        <string>/Users/nfiedler/Applications/Zorigami/server</string>
        <key>WorkingDirectory</key>
        <string>/Users/nfiedler/Applications/Zorigami</string>
        <key>RunAtLoad</key>
        <true/>
        <key>EnvironmentVariables</key>
        <dict>
            <key>DB_PATH</key>
            <string>/Users/nfiedler/Library/Application Support/Zorigami/dbase</string>
            <key>HOST</key>
            <string>0.0.0.0</string>
            <key>PORT</key>
            <string>8000</string>
            <key>RUST_LOG</key>
            <string>info</string>
        </dict>
        <key>StandardErrorPath</key>
        <string>/Users/nfiedler/Library/Application Support/Zorigami/error.log</string>
        <key>StandardOutPath</key>
        <string>/Users/nfiedler/Library/Application Support/Zorigami/output.log</string>
    </dict>
</plist>
#+end_src
**** how to start
#+begin_src shell
launchctl load ~/Library/LaunchAgents/zorigami.plist
#+end_src
*** TODO figure out how to give it permission to access files on macOS
**** not sure this is possible, even Zoom does not do this prior to running
*** TODO database backup procedure should prune old snapshots
**** for local disk case, retain only a few snapshots
**** for cloud case, honor the least expensive retention plan
** Restore to dissimilar hardware
*** TODO Allow setting the configuration to change the identity
*** TODO Test by restoring a backup to a different system
** Dynamic bucket allocation
*** hard-coded value of 128 is pretty low for local pack stores
*** cloud-based pack stores can accommodate many objects per bucket
*** could consider how frequently new packs are created (1 per day vs hundreds)
** Bucket collision and renaming
*** ~prune extra~ and ~find missing~ will mistakenly remove objects from pack stores
**** need to consider the buckets and objects that may have been renamed by the pack store
** Investigate file checksum collision
*** how likely is it for two different files to have the same checksum?
** Manage user passphrase
*** introduce a setup phase in which user is prompted for passphrase
*** store passphrase in the local key store
*** design mechanism for changing the passphrase but keeping old ones for decrypting packs
** Improved interface
*** c.f. https://duplicacy.com/guide.html webui looks good
** Loose backend issues
*** TODO refine use of =&str= and =String= arguments by using =Into<String>=
**** c.f. https://jwilm.io/blog/from-str-to-cow/ for explanation
**** note that using =Cow= helps to minimize copying
#+BEGIN_SRC rust
pub fn name<T: Into<String>>(mut self, name: T) -> Self {
    self.name = Cow::Owned(name.into());
    self
}
#+END_SRC
*** TODO Too many open files (in RocksDB)
**** need to set =set_max_open_files()= on database options
**** default ulimit on macOS is 256, so something less would be ideal
**** ran out of files in tanuki when rocksdb directory contained 217 files
**** maybe consider a means of countering this error at runtime
*** TODO the monthly fuzzy schedule test fails on the 30th of the month
** Loose GraphQL tasks
*** TODO where do errors go?
**** database restore was failing but frontend reported nothing
*** TODO schema custom types need some unit tests
**** especially the schedule validation code
*** TODO probably should use a better client cache
**** c.f. =graphql_flutter= example that implements a =uuidFromObject()= function
**** uses the "type" of the object and its unique identifier as the caching key
**** our objects would need to have a "typename" for this to work
*** TODO find out how to document arguments to mutations
**** c.f. juniper API docs: Attribute Macro juniper::object
** Loose WebUI tasks
*** TODO pack store ~test~ feature shows snackbar repeatedly
*** TODO sometimes get an HTTP error in GraphQL client
**** should automatically retry the query a few times before giving up
*** TODO test with a smaller browser window to surface sizing issues
*** TODO when there are no snapshots, clicking the dataset row does nothing
*** TODO how to refresh the snapshots screen?
**** gets stale as soon as a backup has been run
**** navigation to the snapshots does not work if there were none to start with
**** maybe add a refresh button like in google cloud console
*** TODO schedule start/stop times should be using local time (no excuse for not doing this)
*** TODO local store basepath and google credentials should use file picker
**** https://pub.dev/packages/form_builder_file_picker
*** TODO improve the navigation drawer
**** currently selected option should be highlighted, not actionable
*** TODO improve (server) error handling
**** when a temporary server error occurs, offer a "Retry" button
*** TODO improve snapshot tree browser
**** should sort entries by filename case-insensitively
**** for larger number of entries, should use =PaginatedDataTable=
**** nice to have: sticky table header
**** nice to have: sort by file type
*** TODO consider how to hide the minio secret key using a show/hide button
*** TODO consider approaches to l10n and i18n
**** c.f. https://resocoder.com/2019/06/01/flutter-localization-the-easy-way-internationalization-with-json/
*** TODO improve the data sets form
**** TODO FAB covers the =DELETE= button even when scrolled all the way down
**** TODO use the =validate()= function on =DataSet= to ensure validity
**** TODO should decode the computer ID to improve readability
*** TODO should sort the datasets so they are always in the same order
**** maybe sort them by date, with most recent first
*** TODO tree entries of =ERROR= type should be displayed as such
**** error message from =TreeEntry.new()= could be stored as a new type of =TreeReference=
***** e.g. =TreeReference.ERROR(String)= where the string is the error message
*** TODO should have ui for listing all snapshots in a dataset
**** consider presenting in a style similar to Time Machine
**** e.g. a timeline of the snapshots
**** c.f. https://pub.dev/packages/flutter_timeline
**** probably need paging in the ui and graphql api
*** TODO improve the page for defining stores
**** TODO delete button should be far away from the other button(s)
**** TODO delete button should require two clicks, with "are you sure?"
*** TODO use breadcrumbs in the tree navigator to get back to parent directories
*** TODO consider and improve accessibility
**** enable testing for a11y sanity
**** add hints to improve the presentation of information
***** configuration panel
***** snapshot browser
** Improved error handling
*** Collect and present errors encountered during the backup
**** e.g. all the "permission denied" and such
*** Data set input validation
**** Ask backend to verify the entered basepath before trying to save
*** Pack store input validation
**** should validate Google Cloud service account key when defining pack
*** Consider a structured design for error types and handling
**** c.f. https://fettblog.eu/rust-enums-wrapping-errors/
*** Look at https://github.com/dtolnay/thiserror for defining error types
*** May improve error handling and reporting
** Initial Configuration
*** Walk user through pack store and data set creation
*** Offer path for restoring database from existing pack store
*** Allow user to set user/host names for computer UUID
**** They may need to avoid naming conflicts with other local users
**** Imagine a computer lab all sharing a single cloud storage account
*** Allow user to browse files in existing dataset and choose which ones to exclude
** Remote pack store interaction
*** Remote pack stores like Google Cloud have built-in limits for certain operations
**** need to consider that GCS will limit the number of buckets listed to 1,000
**** probably minio and S3 have similar default limits
**** the API generally offers a means of paging to get everything in chunks
** Remove files/folders from backup
*** Allow removing files from existing backups
**** e.g. accidentally saved large binaries
** Snapshot Pruning
*** Use a multi-phased approach with pruning and garbage collection
*** Must not run collection while a backup is in progress
*** Must prevent a backup from starting while pruning is in progress
*** Phase 1: prune snapshots based on a policy
**** set the child's parent reference to skip over stale snapshot
**** e.g. remove snapshots more then N days old
**** e.g. keep N snapshots per day, M per week, and P per month
*** Phase 2: prune unreachable objects in the database
**** copy everything reachable to a new database instance
****** datasets -> snapshots -> trees -> files -> chunks -> packs
**** delete the old database
*** Phase 3: prune unreferenced packs from pack store
**** honor cloud data retention policies
***** e.g. typically anything older than 90 days costs nothing to delete
***** Google has different minimum storage durations for each storage class
****** https://cloud.google.com/storage/docs/storage-classes
***** user can specify their own value for each pack store if necessary
*** Phase 4: prune old database snapshots (no need to keep old copies)
**** honor cloud data retention policies
**** use the =upload_time= in the =Pack= record to determine age
*** Implementation should follow Clean Architecture to improve testability
**** entities and use case separated from data sources via repositories
**** this allows for easily mocking up data to feed the pruning use case
***** i.e. when the use case asks for trees and such, give it mock data structures
** Advanced Scheduling
*** backend
**** Permit ~hourly~ backups every N hours
**** Permit ~daily~ backups every N days
**** Permit ~weekly~ backups every N weeks
**** Permit ~monthly~ backups every N months
*** frontend
**** TODO Support multiple schedules in interface
**** TODO Support day-of-week in schedule
**** TODO Support day-of-month in schedule
**** TODO Support week-of-month in schedule
**** TODO Support time-range in schedule
** Filters for excluding files by size
*** allow adding rules on a dataset to ignore files that are too small/large
** Point-in-time snapshots
*** Backup procedure is file-by-file, which may yield broken snapshots
**** e.g. database files can change during the backup, leading to invalid snapshots
*** If available, use the OS functionality for FS snapshots
**** ZFS has snapshot support
** More Functionality
*** TODO search snapshots to find a file/directory by a given pattern
**** the file/dir is not in the latest snapshot but some older one, go find it
**** might not even know the full path of the file/dir in question
*** TODO store restore requests in database to tolerate application restart
**** currently restore requests are queued in memory only, so a crash means everything is forgotten
*** TODO Perform a full backup on demand, discard all previous backups
**** Wifey doesn't like the idea of accumulating old stuff
**** Gives the user a chance to save space by removing old content
**** Optionally prune all existing packs in the process
*** TODO event dispatching for the web and desktop
**** use the state management to manage "events" and state
**** engine emits actions/events to the store
***** for backup and restore functions
***** e.g. "downloaded a pack", "uploaded a pack"
**** store holds the cumulative data so late attachers can gather everything
**** supervisor threads register as subscribers to the store
**** clients will use GraphQL subscriptions to receive updates
**** supervisor threads emit GraphQL subscription events
*** TODO consider how datasets can be modified after creation
**** cannot change stores assigned to dataset once there are snapshots
**** basically would require starting over if changing stores, base path, etc
*** TODO Secure FTP improvements
**** TODO support SFTP with private key authentication
***** use store form to take paths for public and private keys
**** TODO allow private key that is locked with a passphrase
***** passphrase for private key would be provided by envar
*** TODO Repair missing pack files in pack stores
**** expose the GraphQL operation via the graphical interface
** More Information
*** TODO track start and finish time for a backup
**** account for time when backup is paused due to schedule
*** TODO show differences between any two snapshots
**** collect the paths and sizes of all new/changed files
**** somehow show all of that information in a scalable fashion
*** TODO show =fileCounts= query for each of latest N snapshots to show recent data growth
*** TODO Show details about snapshots and files
**** show differences between two snapshots
**** show pack/chunk metrics for   all   files in a snapshot
**** show pack/chunk metrics for changed files in a snapshot
*** TODO Query to see histogram of file sizes, number of chunks, etc
**** for a given snapshot
***** count number of files with N chunks for all values of N
*** TODO Show number of packs stored in a pack store
** Database Integrity
*** support database integrity checks
**** ensure all referenced records actually exist
**** like git fsck, start at the top and traverse everything
**** find and report dangling objects
**** an automated scan could be run on occasion
** Pack file integrity
*** Retrieve random pack files and verify integrity
** Architecture Review
*** Rust dependency injection, is it helpful?
**** https://github.com/AzureMarker/shaku
**** https://github.com/mineichen/minfac
**** https://github.com/austinjones/lifeline-rs (whole runtime message bus system)
**** https://github.com/dmitryb-dev/waiter
**** https://github.com/tobni/inject-rs
*** Actor framework review, is actix still good?
**** https://github.com/slawlor/ractor aims to be like Erlang
**** no framework, just tokio: https://ryhl.io/blog/actors-with-tokio/
*** Database per dataset directory
**** Centralized configuration in a known location
***** would default to something sensible in user home directory
***** overridden by environment variable
***** JSON or XML formatted plain text file
***** Holds paths to the various data sets
***** Holds pack store configuration
**** Each data set directory has a database directory (and backup)
**** Backup process automatically excludes the database and its backup
**** What would a full restore procedure look like?
**** Benefits
***** reduced risk in the event of database corruption
**** Drawbacks
***** additional disk usage for database overhead
***** forces user to keep database with the dataset
*** Parallel backups
**** Currently the backup supervisor spawns a single thread (=Arbiter=) to manage backups
**** This causes all backups to be serialized
**** For parallel backups, would use the =SyncArbiter= from actix
*** Database migrations
**** Use the =serde= crate features (c.f. https://serde.rs)
**** Use =#[serde(default)]= on struct to fill in blanks for new fields
**** Add =#[serde(skip_serializing)]= to a deprecated struct field
**** New fields will need accessors that convert from old fields as needed
***** reset the old field to indicate it is no longer relevant
**** Removing a field is no problem for serde
*** Shared pack repository
**** Current design basically forces each user/install to have a separate pack repo
**** Otherwise the pack pruning would delete the packs for other users saving to the same repo
*** Embedded Database
**** Is the default RocksDB performance sufficient?
**** Consider https://github.com/spacejam/sled/
***** written in Rust, open source
***** will need prefix key scanning
****** looks like you just use a prefix of the key (sorts before the matching keys)
***** will need backup/restore functions
*** Client/Server
**** Look at ways to secure the server, to allay fears of exploits
**** A web conferencing tool was exploited via its hidden HTTP server
** Desktop application
*** design a configuration system for desktop
**** define the whole clean architecture setup
***** entities, use cases, repositories
**** data source for web will have values defined by environment_config only
**** data source for desktop will use shared preferences (?) for persistence
**** data layer repository chooses between data sources based on environment
***** how to detect if application was compiled for web
#+BEGIN_SRC dart
import 'package:flutter/foundation.dart' show kIsWeb;
if (kIsWeb) { /* web stuff */ } else { /* not web */ }
#+END_SRC
*** clipboard support
**** look for clipboard plugin for flutter (for macOS)
**** c.f. https://flutter.dev/docs/development/packages-and-plugins/developing-packages
** macOS support
*** TODO optional Time Machine style backup and retention policy
**** hourly backups for 24 hours
**** daily backups for 30 days
**** weekly backups for everything else
**** prune backups to maintain a certain size
** Full Restore
*** Procedure for full restore
**** User installs and configures application
**** User invokes "full restore" function
**** User provides a temporary pack store configuration
**** Query pack store to get candidate computer UUID values
**** User chooses database to restore
***** if current UUID matches one in the available set, select it by default
**** Fetch the most recent database files
***** Restore to a different directory, then copy over records
***** Copy every record except for =configuration= (and maybe others?)
***** Copy records for datasets, stores, snapshots, packs, etc
**** User can now browse datasets and restore as usual
**** Restoring an entire dataset is simply the "tree restore" case
*** Walk the user through the process
**** Configure the primary pack store for retrieval
**** Inform user that this pack store configuration is only temporary
**** Select database to retrieve based on computer UUID
**** Instruct user to restore as usual from dataset(s)
*** TODO Restore symbolic links (currently does nothing with =TreeReference::LINK=)
*** TODO Restore file attributes from tree entry
**** TODO File mode
**** TODO File user/group
**** TODO File extended attributes
*** TODO Restore directories from snapshot
**** TODO Directory mode
**** TODO Directory user/group
**** TODO Directory extended attributes
**** TODO Restore multiple files efficiently
**** TODO Restore a directory tree efficiently
*** TODO Detect and prune stale snapshots that never completely uploaded
**** Stale snapshots exist in the database but are not referenced elsewhere
*** TODO Support snapshots consisting only of mode/owner changes
**** i.e. no file content changes, just the database records
** Windows support
*** TODO Does Microsoft's ~Windows Package Manager~ (=winget=) provide an easier dev setup path?
*** TODO Backup files opened by a running process
**** Normally cannot read files that are opened on Windows
**** See Volume Shadow Copy Services (VSS) for details
*** TODO Support Windows file types
**** ReadOnly
**** Hidden
**** System
** More Better
*** Ransomware protection
**** descriptions of what this means
***** CloudBerry
: CloudBerry Backup detects encryption changes in files and prevents existing
: backups from being overwritten until an administrator confirms if there is an
: issue.
***** Arq:
: Ransomware protection - point-in-time recovery of files
***** https://ruderich.org/simon/notes/append-only-backups-with-restic-and-rclone
: One issue with most backup solutions is that an attacker controlling the local
: system can also wipe its old backups. To prevent this the backup must permit
: append-only backups (also called add-only backups).
****** They change the SSH config to run the backup command with "append only" flag.
*** TODO Permit scheduling upload hours for each day of the week
**** e.g. from 11pm to 6am Mon-Fri, none on Sat/Sun
*** TODO Command-line option to dump database to json (separate by key prefix, e.g. ~chunk~)
*** TODO Support deduplication across multiple computers
**** Place the chunks and packs in a seperate "database" for syncing
***** For RocksDB, use a column family if it helps with =GetUpdatesSince()=
**** RocksDB replication story as of 2019-02-20:
: Q: Does RocksDB support replication?
: A: No, RocksDB does not directly support replication. However, it offers
: some APIs that can be used as building blocks to support replication.
: For instance, GetUpdatesSince() allows developers to iterate though all
: updates since a specific point in time.
***** see =GetUpdatesSince()= and =PutLogData()= functions
**** User configures the host name of the ~peer~ installation
***** Use that to form the URL with which to =sync=
**** Share the chunks and packs documents with a ~peer~ installation
**** At the start of backup, sync with the ~peer~ to get latest chunks/packs
*** TODO Consider how to deal with partial uploads
**** e.g. Minio/S3 has a means of handling these
*** TODO Permit removing a store from a dataset
**** would encourage user to clean up the remote files
**** for local store, could remove the files immediately
**** must invalidate all of the snapshots effected by the missing store
*** TODO Permit moving from one store to another
**** would mean downloading the packs and uploading them to the new store
*** TODO Consider how to backup and restore FIFO, BLK, and CHR "files"
**** c.f. https://github.com/jborg/attic/blob/master/attic/archive.py
**** c.f. https://github.com/avz/node-mkfifo (for FIFO)
**** c.f. https://github.com/mafintosh/mknod (for BLK and CHR)
* Product
** Why another backup program?
*** Original reason in 2014: nothing supported Linux and Glacier
*** What will set this apart from other offerings?
**** Easy to use graphical interface (both desktop and web)
**** Cross-platform (Windows, macOS, Linux)
**** Support for multiple backends, including Glacier
** TODO Define an MVP and work toward release
** TODO Evaluate other backup software
*** TODO Check out some on App Store
**** Backup Guru LE
**** ChronoSync Express
**** Backup
**** Remote Backup Magic
**** Sync - Backup and Restore
**** Backup for Dropbox
**** Freeze - for Amazon Glacier
*** Lot of "folder sync" apps out there
** TODO Define the target audience
*** Average home user, no technical expertise required
** TODO Need distinquishing features
*** What sets this application apart from the other polished products?
**** Cross-platform (e.g. macOS, Windows)
**** Linux server ready
** Application Monitoring
*** Need something to capture failures for debugging
**** c.f. https://sentry.io/welcome/
** Windows Certified
*** CloudBerry(?) has bunches of certifications
*** is that really so meaningful? *I* never cared
** Alternatives
*** Commercial
**** Arq
***** https://www.arqbackup.com
***** Windows, Mac
***** Uses a single master password
***** Supports numerous backends
**** Carbonite
***** https://www.carbonite.com
***** Consumer and business
***** Billed monthly
***** 128-bit encryption for all but most expensive plan
***** Windows, Mac, and "servers"
***** Seems to backup to their servers
**** CloudBerry
***** https://www.cloudberrylab.com/backup
***** Consumer and business
***** Windows, Mac, Linux
***** Supports Glacier and other services
***** Freeware version lacks compression, encryption, limited to 200GB
**** Duplicacy
***** https://github.com/gilbertchen/duplicacy
***** Lists other open source tools and compares them
***** Deduplicates chunks across systems
***** Does not use a database supposedly
***** Does not and can not support Glacier
**** JungleDisk
***** https://www.jungledisk.com/encrypted-backups/
***** Primarily business oriented
***** Seems to rely on their servers
***** Probably stores data elsewhere
**** Rebel Backup
***** https://www.svsware.com/rebelbackup
***** Encrypted backups to Dropbox or Google Drive
***** macOS only
**** qBackup
***** https://www.qualeed.com/en/qbackup/
***** Windows, Mac, Linux
***** Supports numerous backends
***** Has copious documentation with screen shots
**** tarsnap
***** https://www.tarsnap.com
***** Free client
***** Uses public key encryption rather than a password
***** Stores data in Amazon S3
***** Relies on tarsnap servers
***** 10x the price of Google Cloud or Amazon Glacier
***** Command-line interface
*** Open Source
**** Asuran
***** https://gitlab.com/asuran-rs/asuran/
***** a library and cli for making backups
***** stores all chunks in content-addressable stores
**** Attic
***** https://attic-backup.org
***** Development stopped in 2015
***** Only supports SSH remote host
***** Command-line interface
**** Borg
***** https://borgbackup.readthedocs.io/
***** Fork of Attic
***** Only supports SSH remote host
***** Command-line interface
**** bup
***** https://bup.github.io
***** Git-like (uses Python and Git) pack file storage
***** Requires a bup server for remote storage
***** Command-line interface
**** Duplicati
***** https://www.duplicati.com/
***** Requires .NET or Mono
***** Web-based interface
***** Windows and Linux
***** No Glacier support
**** duplicity
***** http://duplicity.nongnu.org
***** Uses GnuPG, a tar-like format, and rsync
***** Supports backends with a filesystem-like interface
***** Command-line interface
***** No Glacier support
**** rclone
***** https://github.com/rclone/rclone
***** Syncs a directory structure to the cloud
***** Offers chunking and encryption
***** Command-line interface
***** No Glacier support
**** restic
***** https://restic.net
***** Git-like data model
***** Supports numerous backends (no Glacier)
***** Command-line interface
** Name
*** Joseph suggests "Attic"
**** =atticapp.com= is taken
**** =attic.app= is for sale
**** Look for ~attic~ in different languages
**** Esperanto: ~mansardo~
***** also means something in Macedonian
**** Hawaiian: ~kaukau~
**** Latin: ~atticae~
* Documentation
** Duplicati has a fun description of how the backup works
*** files are broken into "bricks" which go in "bags" and stored in big "boxes" (the pack store)
*** c.f. https://duplicati.readthedocs.io/en/latest/01-introduction/
** TODO Third party license attributions
*** Include any/all third party license attribution somewhere
*** =cargo lichking bundle= will dump everything to the console
** TODO document how the user might change the passphrase over time
*** user must remember their old passwords in order to decrypt old pack files
*** the application will never store the actual password anywhere
*** will need to prompt the user when a different passphrase is needed
* Technical Information
** Parallelism
*** shortening snapshot time
**** mini parallelism is 8
**** server parallelism is 4
**** mini before: 555706 files after 3 minutes 2 seconds (original)
**** mini after: 556625 files after 1 minutes 27 seconds (initial)
**** mini after: 571056 files after 1 minutes 23 seconds (subsequent)
**** server before: 147769 files after 1 hours 20 minutes (original)
**** server after: 147769 files after 52 minutes (initial)
**** server after: 148142 files after 52 minutes (subsequent)
** Error Handling
*** what happens to file errors during scanning?
**** any errors during scan result in the entry being completely ignored
**** they will be processed again on the next scan
*** what happens to file errors during packing?
**** if metadata or opening file fails, recorded as having zero length
**** if packing file fails, overall backup will fail
*** what happens when file contents change between scanning and packing?
**** changed file is stored using the original checksum
**** file will subsequently be backed up again next time
** Data Growth
*** main server
**** better pack file generation
***** average pack size ~before~ change: 68,647,434
***** average pack size ~after~ change: 67,688,886
**** original database schema
***** compressed database seems to grow 8mb in 6 months
***** compressed database size: 61,920,768
**** record counts over time
***** as of 2022-03-17
| entity    |  count |
|-----------+--------|
| snapshots |    576 |
| files     | 137081 |
| trees     |  97598 |
| chunks    | 190758 |
| packs     |   4282 |
| xattrs    |  19263 |
***** as of 2023-01-14
| entity    |  count |
|-----------+--------|
| snapshots |    272 |
| files     | 134950 |
| trees     |  65312 |
| chunks    |  56813 |
| packs     |   4107 |
| xattrs    |  18035 |
*** mac mini statistics
**** original database schema
***** 2022-03-15
****** compressed database size: 2,379,181,138
****** database record counts
| entity    |   count |
|-----------+---------|
| snapshots |     190 |
| files     | 1806620 |
| trees     |  302860 |
| chunks    | 1830167 |
| packs     |    3380 |
| xattrs    |  185473 |
**** with file/chunk record optimization
***** 2022-03-18
****** compressed database size: 454,232,580
****** database record counts
| entity    |  count |
|-----------+--------|
| snapshots |      1 |
| files     | 412555 |
| trees     |  48927 |
| chunks    |   4025 |
| packs     |    369 |
| xattrs    |  14388 |
****** only 1% of files are larger than a chunk
**** with new packing algorithm
***** 2022-03-22
****** compressed database size: 462,090,768
****** database record counts
| entity    |  count |
|-----------+--------|
| snapshots |      1 |
| files     | 457980 |
| trees     |  52844 |
| chunks    |   4061 |
| packs     |    176 |
| xattrs    |  13624 |
**** better pack file generation
***** average pack size ~before~ change: 46,960,186
***** average pack size ~after~ change: 70,496,178
**** very small files in database
***** 2022-03-26
****** compressed database size: 463,418,613
****** database record counts
| entity    |  count |
|-----------+--------|
| snapshots |      1 |
| files     | 437775 |
| trees     |  53499 |
| chunks    |   4185 |
| packs     |    188 |
| xattrs    |  12808 |
****** file counts
| type           |  count |
|----------------+--------|
| directories    |  73536 |
| symlinks       |  14543 |
| filesBelow80   |  56407 |
| filesBelow1k   | 293740 |
| filesBelow10k  | 351019 |
| filesBelow100k |  88622 |
| filesBelow1m   |  11461 |
| filesBelow10m  |   2322 |
| filesBelow100m |    222 |
| veryLargeFiles |     16 |
**** working file excludes
***** 2022-03-27
****** compressed database size: 70,466,060
****** database record counts
| entity    |  count |
|-----------+--------|
| snapshots |      1 |
| files     | 321419 |
| trees     |  40786 |
| chunks    |   1533 |
| packs     |     67 |
| xattrs    |    350 |
****** file counts
| type           |  count |
|----------------+--------|
| directories    |  45074 |
| symlinks       |    672 |
| filesBelow80   |  52633 |
| filesBelow1k   | 135193 |
| filesBelow10k  | 311948 |
| filesBelow100k |  65487 |
| filesBelow1m   |   5531 |
| filesBelow10m  |    789 |
| filesBelow100m |    164 |
| veryLargeFiles |      7 |
** Pack files
*** analysis of overly large pack files before accounting for tar entry overhead
**** packing would only account for compressed size of chunks
**** with many small files, tar file overhead increased file size by half (99mb vs 64mb)
| pack digest | count |  file sz | content len | smallest | largest | average |
|-------------+-------+----------+-------------+----------+---------+---------|
| 3fa54d0     | 19193 | 82480128 |    67114835 |       22 | 4755936 |    3496 |
| b93402d     | 39932 | 99137536 |    67109129 |       39 |  446087 |    1680 |
| c57960e     | 38894 | 98344448 |    67111246 |       40 |  452424 |    1725 |
| ef6ff7a     | 40001 | 99184640 |    67111284 |       40 |  492592 |    1677 |
** Possible corner cases
*** Database backup, then restore, then pack prune
Because the database snapshot is recorded in the database after the snapshot
has already been uploaded, if the user were to restore the database and then
perform a pack pruning, the most recent database snapshot would be removed.
** JS Build Artifacts
*** Flutter => main.dart.js
| State      |    Size |
|------------+---------|
| production | 1742125 |
*** ReasonML + Webpack => main.js
| State       |    Size |
|-------------+---------|
| development | 2761882 |
| production  |  536345 |
| gzipped     |  145785 |
** Exploring other languages
*** Compile to native for easy deployment
*** Compile to native for code obfuscation
*** Rust
**** Advantages
***** compile to native
***** expressive, safe type system
***** good dependency management
***** lots of useful tools (e.g. clippy)
**** Disadvantages
***** fewer libraries compared to Go
*** Go vs Rust
**** Go: first class support for cloud services
**** Go: statically linked OpenPGP readily available
**** Go: easy to read and write language
**** Rust: mature dependency management tooling
**** Rust: cargo has good editor support
**** Rust: expressive type system
**** Rust: nominal subtyping is much easier to follow
**** Rust: streamlined error handling
**** Rust: fine-grained namespaces and visibility control
* Deployments
** local test
*** setup
**** base path: =/Users/nfiedler/projects/zorigami=
**** excludes: =.git, **/tmp, **/.tmp, **/target=
**** run hourly
** mac mini
*** how to build: see =README.md= in ~Local Test Build~ section
*** how to update
1) =ps -ef | grep -i zorigami=
2) stop: =launchctl kill SIGTERM gui/501/zorigami=
3) =mv target/release/server ~/Applications/Zorigami=
4) =rsync -vcr build/web ~/Applications/Zorigami/=
5) enable: =launchctl enable gui/501/zorigami=
6) start: =launchctl kickstart -p gui/501/zorigami=
7) =ps -ef | grep -i zorigami=
*** may need to =enable= and =kickstart= twice due to code signing error
#+begin_src
Termination Reason: CODESIGNING 4 Launch Constraint Violation
#+end_src
*** setup
**** launch agent in =~/Library/LaunchAgents/zorigami.plist=
**** database in =~/Library/Application\ Support/Zorigami=
**** pack store: =/Volumes/Zbackup=
**** base path: =/Users/nfiedler=
**** excludes: =.Trash, Library, **/Downloads, **/node_modules, **/target=
**** run hourly
** kohaku
*** deploy using docker and ansible
*** store credentials file as =/zeniba/shared/zorigami.json=
*** data sets
**** shared files
:base_path: /datasets
:excludes: (none)
:pack_size: 64mb
:pack_stores: all
:schedule: daily, 11:30pm to 5:30am
*** pack stores
**** local
:label: safe
:path: /packstore
**** cloud
:label: google
:credentials: /datasets/zorigami.json
:project_id: zorigami-376823
:region: us-west1
:storage: NEARLINE
