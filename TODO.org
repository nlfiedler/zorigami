* Zorigami
** Phase 2
*** DONE Find out average file size on macOS
**** For ~nfiedler~ home directory:
***** 330,592 total files
***** 330,213 files under 4 MB
***** average file size: 198 KB
***** files 100 MB to 1 GB: 11
***** files 1 GB to 10 GB: 4
***** 3 largest files: 14, 14, 16 GB
**** For ~p4nathan~ home directory:
#+BEGIN_SRC shell
$ gfind . -type f -fprintf /tmp/sizes '%k\n'

$ awk '{ total += $1; count++ } END { print total/count }' /tmp/sizes
580.741

$ sort -n /tmp/sizes | uniq -c
$ awk '{ total += $1 } END { print total }' /tmp/4kfiles.txt
#+END_SRC
***** 145,051 total files
***** 144,768 files under 4 MB
***** average file size: 580 KB
***** files 100 MB to 1 GB: 3
***** files 1 GB to 10 GB: 9
***** 3 largest files: 10, 17, 18 GB
*** DONE Find out average file sizes in the shared data set
**** =/zeniba/shared=
***** 124,381 total files
***** 122,354 files under 4 MB
***** average file size: 2473 KB
***** files 100 MB to 1 GB: 529
***** files 1 GB to 10 GB: 16
***** 3 largest files: 6, 6, 34 GB
*** DONE Find out file sizes for LevelDB
**** logs grow to 4 MB
**** db files grow to 2 MB
*** DONE Consider if PouchDB/LevelDB will be adequate for tracking 300,000 files
**** DONE Try testing with some simple live data test
***** 334,147 files in total
***** 230,219 unique files yields 73 MB of LevelDB data
***** runs in a 8 minutes
*** TODO Write the logic for the "engine"
**** DONE Store the ~encryption~ record in PouchDB
**** DONE Need a strategy for scanning file tree without blowing up
**** DONE Use =bin/dbfiles.js= as a guide for the above ~tree walker~
***** DONE Use =async= and =await= in a single function to process tree
***** DONE Need to return both files and directories
**** DONE Move the =dbfiles.ts= into =engine.ts= and start testing
**** DONE Compute the difference from the previous snapshot
**** TODO Specifically exclude the database from the initial data set
**** DONE Produce a ~working~ snapshot of the dataset
**** DONE Get the user and group names in addition to uid, gid
***** look for =getpwuid= and =getgrgid= support
***** https://github.com/prantlf/node-posix-ext
**** DONE Get the xattrs and store in database (deduplicate)
**** DONE Detect symbolic links and store their reference
**** TODO Produce and upload pack files containing new/changed files
***** TODO Fill a queue of N file objects to allow fitting chunks to packs
(but without having all files in memory at once, which could be many the first time)
****** Maybe =EventEmitter= makes sense here, too, to allow reporting errors
***** TODO Split large files across packs
*** TODO Add =license-checker= and run it
*** TODO Consider if =Chunk= should have its own module, and more functions
*** TODO Consider if =database= module should define a =Document= type
*** TODO Update the architecture and data model in =NOTES.md=
*** TODO Consider adding =deduplication= definitions to DefinitelyTyped
**** c.f. http://definitelytyped.org/guides/contributing.html
*** DONE Check out the documentation for =tsconfig.json= and compiler options
*** TODO Restore files from snapshot
**** TODO File content
**** TODO File mode
**** TODO File user/group
**** TODO File extended attributes
**** TODO Directory mode
**** TODO Directory user/group
**** TODO Directory extended attributes
** Phase 3
*** TODO Use starter [[https://github.com/Microsoft/TypeScript-Node-Starter][guide]] to get Node set up with TypeScript
***** TODO Look more at how https://github.com/TypeStrong/ts-node can be used
***** TODO Is rewriting =app.js= worthwhile or necessary?
***** TODO Translate the routes
*** TODO Maybe rewrite =gulpfile.ts= in TypeScript
***** c.f. https://github.com/TypeStrong/ts-node
***** https://github.com/vvakame/typescript-project-sample/blob/master/gulpfile.ts
*** TODO Introduce GraphQL backend and schema
**** TODO Define the schema
**** TODO Write a simple resolver
**** TODO Write a unit test
*** TODO Write a ReasonML frontend
**** TODO Add =bs-platform= dependency and =bsconfig.json= file
**** TODO Put front-end code in a directory named =web-src=
**** TODO Set up =gulp= and =webpack= to build the front-end code
**** TODO Set up the routing
**** TODO Write a simple home page that shows something
** Phase 4
*** TODO Use this to replace =replicaz= by persisting over SFTP
*** TODO Design the cloud interface code to be service agnostic
**** Same basic plugin design as the stores
*** TODO Store database in a bucket named after the "computer UUID"
*** TODO Store pack files in Google Cloud Storage
- https://github.com/googleapis/nodejs-storage/
** Phase 5
*** TODO Support snapshots consisting only of mode/owner changes
**** i.e. no file content changes, just the database records
*** TODO Support deduplication across multiple computers
**** Place the chunks and packs in a seperate "database" for syncing
**** Use the express support in [[https://github.com/pouchdb/pouchdb-server][pouchdb-server]] to serve up chunks/packs db
**** User configures the host name of the ~peer~ installation
***** Use that to form the URL with which to =sync=
**** Share the chunks and packs documents with a ~peer~ installation
**** At the start of backup, sync with the ~peer~ to get latest chunks/packs
*** TODO Store pack files in Amazon Glacier
**** c.f. https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/welcome.html
**** Offer user option to use "expedited" retrievals so they go faster

*** TODO Automatically prune backups more then N months old
**** For Google and Amazon, anything older than 90 days is free to remove
**** This would be a configuration setting, with defaults and path-specific
* Electron App
** Phase N
*** TODO Write it in TypeScript
*** TODO Create a system tray icon/widget
**** Popup menu like Time Machine
**** Show current status, last backup
**** Action to open the app and examine snapshots
**** Action to open the app and check settings
* Product
** Name
*** Joseph suggests "Attic"
**** =atticapp.com= is taken
**** =attic.app= is for sale
**** Look for ~attic~ in different languages
