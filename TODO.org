* Tasks
** TODO replace =lazy_static= with new =LazyLock= in std
** Restore improvements
*** request being processed needs to be returned by =requests()= somehow
**** it is neither ~pending~ nor ~completed~ so it is not returned in queries
**** if the restore supervisor crashes, the in-progress request is lost
**** add a new type that manages the ~pending~ and ~completed~ lists
***** have =Restorer= create this type and pass a =Arc<Mutex>= to supervisor
***** add function for moving the first ~pending~ request to ~processing~
***** add function for updating status of ~processing~ request
***** add function for moving ~processing~ request to the ~completed~ set
***** add function like =requests()= that combines all 3 sets into one for queries
*** restore can hang on pack retrieval for a long time
**** log output
#+begin_src
[2023-08-08T15:34:54Z DEBUG server::domain::managers::restore] fetching pack sha256-9624d9d7cb9dfc37740c121fdbe15f456b6b2153d5b46b3e43feea1cd8db5cbf
[... 15 minutes later ...]
[2023-08-08T15:51:20Z WARN  server::data::repositories] pack retrieval failed, will try another source: Err(Token retrieval failed: error trying to connect: Connection timed out (os error 110))
#+end_src
**** should try to set a timeout to something reasonble (1-2 minutes)
*** add a =status_msg= property to =Request= for showing detailed progress
**** normally show the current path of the file being restore
**** if very large file, show ~fetching very large file~
**** if waiting for a long time for a pack file, indicate in status
*** add the request status field to the graphql entity
*** show the request status message in the web ui
*** web ui should refresh restore requests page every few seconds
** Database Integrity
*** support database integrity checks
**** ensure all referenced records actually exist
**** like git fsck, start at the top and traverse everything
**** find and report dangling objects
**** TODO finish writing =verify_snapshot= use case tests
**** TODO add GraphQL query to invoke =VerifySnapshot= usecase
** Improve backup reliability
*** consider capturing all failed file backups in the database
**** attempt to process the failed files during subsequent backups
*** run a verification step after the backup is complete
*** add some ~debug~ logging at regular intervals during the scan phase
*** report status of the backup (e.g. ~scanning~, ~packing~, ~verifying~)
**** should show the start time, not simply ~still running~
**** show number of files packed, number of packs uploaded (what info logging shows now)
**** capture actual time spent in each phase, accounting for ~pause~ times
** AWS SDK for Rust now available
*** c.f. https://aws.amazon.com/blogs/developer/announcing-general-availability-of-the-aws-sdk-for-rust/
** Efficient restore
*** if restore finds an existing workspace, scan contents to build a ~have~ list of chunks vs starting over
*** if restoring a file over an existing target, skip if checksum of target matches records
**** an integration test exists in =restore_manager_test.rs= that is commented out
** Partial restore
*** using latest snapshot, examine current data set and restore all missing/modified data
*** provide lots of logging and frequent webui status updates
** Snapshot browsing
*** The whole display of snapshots needs to be improved
*** improve snapshot tree browser
**** should sort entries by filename case-insensitively
**** for larger number of entries, should use =PaginatedDataTable=
**** nice to have: sticky table header
**** nice to have: sort by file type
*** Stop using the list item view thing and the leading/trailing widgets
*** Make sure buttons actually look like buttons
*** Have fields for start time, stop time, current status, additional details, etc
*** scrolling snapshot directory with lots of entries (239) sometimes becomes unresponsive
**** scrolling no longer responds
**** navigation, selection no longer respond
**** probably an error is ocurring and it's invisible in production
** Restore to dissimilar hardware
*** Allow setting the configuration to change the identity
*** Test by restoring a backup to a different system
** Dynamic bucket allocation
*** hard-coded value of 128 is pretty low for local pack stores
*** cloud-based pack stores can accommodate many objects per bucket
*** could consider how frequently new packs are created (1 per day vs hundreds)
** Bucket collision and renaming
*** ~prune extra~ and ~find missing~ will mistakenly remove objects from pack stores
**** need to consider the buckets and objects that may have been renamed by the pack store
** Manage user passphrase
*** introduce a setup phase in which user is prompted for passphrase
*** store passphrase in the local key store
*** consider how to change the passphrase but retain old ones for decrypting packs
** Make error message text selectable/copyable
*** file restore status shows an error message that cannot be selected and copied to the clipboard
** Loose backend issues
*** neat way to getting filenames in a streamlined manner
**** c.f. https://fettblog.eu/refactoring-rust-abstraction-newtype/
*** should clean up dataset workspaces on startup and periodically
**** need to be sure no backup or restore is running, then delete everything in =.tmp=
**** =State= could have a =is_quiet()= check or an event that be be subscribed to when everything is quiet
*** refine use of =&str= and =String= arguments by using =Into<String>=
**** c.f. https://jwilm.io/blog/from-str-to-cow/ for explanation
**** note that using =Cow= helps to minimize copying
#+BEGIN_SRC rust
pub fn name<T: Into<String>>(mut self, name: T) -> Self {
    self.name = Cow::Owned(name.into());
    self
}
#+END_SRC
*** Too many open files (in RocksDB)
**** need to set =set_max_open_files()= on database options
**** default ulimit on macOS is 256, so something less would be ideal
**** ran out of files in tanuki when rocksdb directory contained 217 files
**** maybe consider a means of countering this error at runtime
*** the monthly fuzzy schedule test fails on the 30th of the month
*** eventually switch from =serde_cbor= to https://crates.io/crates/ciborium
**** the docs have zero examples, no idea how to use the API
** Loose GraphQL tasks
*** where do errors go?
**** database restore was failing but frontend reported nothing
*** schema custom types need some unit tests
**** especially the schedule validation code
*** probably should use a better client cache
**** c.f. =graphql_flutter= example that implements a =uuidFromObject()= function
**** uses the "type" of the object and its unique identifier as the caching key
**** our objects would need to have a "typename" for this to work
*** find out how to document arguments to mutations
**** c.f. juniper API docs: Attribute Macro juniper::object
** Loose WebUI tasks
*** pack store ~test~ feature shows snackbar repeatedly
*** sometimes get an HTTP error in GraphQL client
**** should automatically retry the query a few times before giving up
*** test with a smaller browser window to surface sizing issues
*** when there are no snapshots, clicking the dataset row does nothing
*** local store basepath and google credentials should use file picker
**** https://pub.dev/packages/form_builder_file_picker
*** improve the navigation drawer
**** currently selected option should be highlighted, not actionable
*** improve (server) error handling
**** when a temporary server error occurs, offer a "Retry" button
*** consider how to hide the minio secret key using a show/hide button
*** consider approaches to l10n and i18n
**** c.f. https://resocoder.com/2019/06/01/flutter-localization-the-easy-way-internationalization-with-json/
*** improve the data sets form
**** TODO FAB covers the =DELETE= button even when scrolled all the way down
**** TODO use the =validate()= function on =DataSet= to ensure validity
**** TODO should decode the computer ID to improve readability
*** should sort the datasets so they are always in the same order
**** maybe sort them by date, with most recent first
*** tree entries of =ERROR= type should be displayed as such
**** error message from =TreeEntry.new()= could be stored as a new type of =TreeReference=
***** e.g. =TreeReference.ERROR(String)= where the string is the error message
*** should have ui for listing all snapshots in a dataset
**** consider presenting in a style similar to Time Machine
**** e.g. a timeline of the snapshots
**** c.f. https://pub.dev/packages/flutter_timeline
**** probably need paging in the ui and graphql api
*** improve the page for defining stores
**** TODO delete button should be far away from the other button(s)
**** TODO delete button should require two clicks, with "are you sure?"
*** use breadcrumbs in the tree navigator to get back to parent directories
*** consider and improve accessibility
**** enable testing for a11y sanity
**** add hints to improve the presentation of information
***** configuration panel
***** snapshot browser
** Improved error handling
*** webui: database restore fails to get archive, should display a sensible error
**** cause 1: include mismatching instance identifier
**** cause 2: wrong user owns the files
**** response from backend looks like:
#+begin_src javascript
{
  "data": null,
  "errors": [
    {
      "message": "database archive retrieval",
      "locations": [
        {
          "line": 2,
          "column": 3
        }
      ],
      "path": [
        "restoreDatabase"
      ]
    }
  ]
}
#+end_src
*** webui: change =ServerFailure= to capture original error cause, not just as a string
**** add factory function that detects common types of errors and produces more helpful failure messages
**** e.g. backend is not responding on home screen, shows ugly stack trace
*** Detect cloud credential issues and display friendly message
**** errors from cloud providers can be cryptic, need to detect and decipher for the user
*** Collect and present errors encountered during the backup
**** e.g. all the "permission denied" and such
*** Data set input validation
**** Ask backend to verify the entered basepath before trying to save
*** Pack store input validation
**** should validate Google Cloud service account key when defining pack
*** Consider a structured design for error types and handling
**** c.f. https://fettblog.eu/rust-enums-wrapping-errors/
*** Look at https://github.com/dtolnay/thiserror for defining error types
** Remote pack store interaction
*** Remote pack stores like Google Cloud have built-in limits for certain operations
**** need to consider that GCS will limit the number of buckets listed to 1,000
**** probably minio and S3 have similar default limits
**** the API generally offers a means of paging to get everything in chunks
** Remove files/folders from backup
*** Allow removing files from existing backups
**** e.g. accidentally saved large binaries
** TODO [#A] Snapshot Pruning
*** Retention policy is per dataset, normally defaults to ~all~
*** TODO test: =EntityRepository= with empty main datasource and populated fallback
*** DONE Entity repository will have two database references from now on
**** A: read-first/write-first database
**** B: fallback-read database, if present
**** read: if record missing from A, check B
**** write: always go to A
**** normally B is an =Option= and hence not consulted
*** Phase 1: set up new database, have entity repo set fallback reference
*** Phase 2: copy various records from fallback to primary [0/6]
- [ ] configuration
- [ ] computer_id
- [ ] datasets
- [ ] latest_snapshot
- [ ] databases
- [ ] stores
*** Phase 3: make snapshots disappear based on a policy
**** e.g. retain snapshots for N days
**** e.g. retain N snapshots
*** Phase 4: prune unreachable objects in the database
**** copy everything reachable to a new database instance
***** datasets -> snapshots -> trees -> files -> chunks -> packs
***** also files -> xattrs
***** do not bother deserializing the records, just copy the raw bytes
***** add new =get/set= functions on record repository that read/write raw bytes
*** Phase 5: have entity repo clear fallback reference, delete database
*** add logging to show progress and results of pruning
*** temporarily use a shorter default retention value until Leptos rewrite is done
*** finish writing ~Database Integrity~ implementation
** Pack file pruning
*** find unreferenced pack files and remove
*** honor cloud data retention policies
**** e.g. typically anything older than 90 days costs nothing to delete
**** Google has different minimum storage durations for each storage class
***** https://cloud.google.com/storage/docs/storage-classes
**** user can specify their own value for each pack store if necessary
** Database backup pruning
*** remove old database snapshots, no need to keep old copies
*** honor cloud data retention policies
*** use the =upload_time= in the =Pack= record to determine age
** Advanced Scheduling
*** backend
**** Permit ~hourly~ backups every N hours
**** Permit ~daily~ backups every N days
**** Permit ~weekly~ backups every N weeks
**** Permit ~monthly~ backups every N months
*** frontend
**** Support multiple schedules in interface
**** Support day-of-week in schedule
**** Support day-of-month in schedule
**** Support week-of-month in schedule
**** Support time-range in schedule
** Filters for excluding files by size
*** allow adding rules on a dataset to ignore files that are too small/large
** Point-in-time snapshots
*** Backup procedure is file-by-file, which may yield broken snapshots
**** e.g. database files can change during the backup, leading to invalid snapshots
*** If available, use the OS functionality for FS snapshots
**** ZFS has snapshot support
**** APFS has snapshots, not sure what they are exactly
***** how to determine that FS supports this feature?
***** use =tmutil= to create, mount, and remove snapshots
#+begin_src shell
$ tmutil localsnapshot
Created local snapshot with date: 2021-04-05-162425

$ tmutil listlocalsnapshots /
Snapshots for volume group containing disk /:
com.apple.TimeMachine.2021-04-05-162416.local
com.apple.TimeMachine.2021-04-05-162425.local

# mount a specific snapshot
mkdir -p /tmp/snapshot
mount_apfs -s com.apple.TimeMachine.2021-04-05-162416.local / /tmp/snapshot
open /tmp/snapshot

$ tmutil deletelocalsnapshots 2021-04-05-162416
Deleted local snapshot '2021-04-05-162416'
#+end_src
** More Functionality
*** TODO search snapshots to find a file/directory by a given pattern
**** the file/dir is not in the latest snapshot but some older one, go find it
**** might not even know the full path of the file/dir in question
*** TODO store restore requests in database to tolerate application restart
**** currently restore requests are queued in memory only, so a crash means everything is forgotten
*** TODO Perform a full backup on demand, discard all previous backups
**** Wifey doesn't like the idea of accumulating old stuff
**** Gives the user a chance to save space by removing old content
**** remove all records that are _not_ stores and datasets
- latest/
- chunk/
- pack/
- file/
- xattr/
- dbase/
- tree/
**** Optionally prune all existing packs in the process
*** TODO event dispatching for the web and desktop
**** use the state management to manage "events" and state
**** engine emits actions/events to the store
***** for backup and restore functions
***** e.g. "downloaded a pack", "uploaded a pack"
**** store holds the cumulative data so late attachers can gather everything
**** supervisor threads register as subscribers to the store
**** clients will use GraphQL subscriptions to receive updates
**** supervisor threads emit GraphQL subscription events
*** TODO consider how datasets can be modified after creation
**** cannot change stores assigned to dataset once there are snapshots
**** basically would require starting over if changing stores, base path, etc
*** TODO Secure FTP improvements
**** SFTP is twice as slow as MinIO, should investigate why
**** TODO support SFTP with private key authentication
***** use store form to take paths for public and private keys
**** TODO allow private key that is locked with a passphrase
***** passphrase for private key would be provided by envar
*** TODO Repair missing pack files in pack stores
**** expose the GraphQL operation via the graphical interface
** More Information
*** TODO track start and finish time for a backup
**** account for time when backup is paused due to schedule
*** TODO show differences between any two snapshots
**** collect the paths and sizes of all new/changed files
**** somehow show all of that information in a scalable fashion
*** TODO show =fileCounts= query for each of latest N snapshots to show recent data growth
*** TODO Show details about snapshots and files
**** show differences between two snapshots
**** show pack/chunk metrics for   all   files in a snapshot
**** show pack/chunk metrics for changed files in a snapshot
*** TODO Query to see histogram of file sizes, number of chunks, etc
**** for a given snapshot
***** count number of files with N chunks for all values of N
*** TODO Show number of packs stored in a pack store
** Architecture Review
*** document this somewhere: https://gist.github.com/quad/bc2351e2df4a4a815f8e0d19f36cfa80
*** Alternative databases
**** try using a different database and benchmark backup, database integrity checks, etc
**** sample data: =reassign_packs= updated ~4214~ pack records in ~2~ seconds
*** Rust dependency injection, is it helpful?
**** https://github.com/AzureMarker/shaku
**** https://github.com/p0lunin/teloc
**** https://github.com/hampusmat/syrette
**** https://github.com/mineichen/minfac
**** https://github.com/austinjones/lifeline-rs (whole runtime message bus system)
**** https://github.com/dmitryb-dev/waiter
**** https://github.com/tobni/inject-rs
*** Actor framework review, is actix still good?
**** https://github.com/slawlor/ractor aims to be like Erlang
**** no framework, just tokio: https://ryhl.io/blog/actors-with-tokio/
*** Flutter/Rust bridge, call Rust from Dart
**** c.f. https://github.com/fzyzcjy/flutter_rust_bridge
**** c.f. https://www.zaynetro.com/post/flutter-rust-bridge-2023/
*** Database per dataset directory
**** Centralized configuration in a known location
***** would default to something sensible in user home directory
***** overridden by environment variable
***** JSON or XML formatted plain text file
***** Holds paths to the various data sets
***** Holds pack store configuration
**** Each data set directory has a database directory (and backup)
**** Backup process automatically excludes the database and its backup
**** What would a full restore procedure look like?
**** Benefits
***** reduced risk in the event of database corruption
**** Drawbacks
***** additional disk usage for database overhead
***** forces user to keep database with the dataset
*** Parallel backups
**** Currently the backup supervisor spawns a single thread (=Arbiter=) to manage backups
**** This causes all backups to be serialized
**** For parallel backups, would use the =SyncArbiter= from actix
*** Database migrations
**** Use the =serde= crate features (c.f. https://serde.rs)
**** Use =#[serde(default)]= on struct to fill in blanks for new fields
**** Add =#[serde(skip_serializing)]= to a deprecated struct field
**** New fields will need accessors that convert from old fields as needed
***** reset the old field to indicate it is no longer relevant
**** Removing a field is no problem for serde
*** Shared pack repository
**** Current design basically forces each user/install to have a separate pack repo
**** Otherwise the pack pruning would delete the packs for other users saving to the same repo
*** Embedded Database
**** Is the default RocksDB performance sufficient?
**** Consider https://github.com/spacejam/sled/
***** written in Rust, open source
***** will need prefix key scanning
****** looks like you just use a prefix of the key (sorts before the matching keys)
***** will need backup/restore functions
*** Client/Server
**** Look at ways to secure the server, to allay fears of exploits
**** A web conferencing tool was exploited via its hidden HTTP server
** Full Restore
*** Procedure for full restore
**** User installs and configures application
**** User invokes "full restore" function
**** User provides a temporary pack store configuration
**** Query pack store to get candidate computer UUID values
**** User chooses database to restore
***** if current UUID matches one in the available set, select it by default
**** Fetch the most recent database files
***** Restore to a different directory, then copy over records
***** Copy every record except for =configuration= (and maybe others?)
***** Copy records for datasets, stores, snapshots, packs, etc
**** User can now browse datasets and restore as usual
**** Restoring an entire dataset is simply the "tree restore" case
*** Walk the user through the process
**** Configure the primary pack store for retrieval
**** Inform user that this pack store configuration is only temporary
**** Select database to retrieve based on computer UUID
**** Instruct user to restore as usual from dataset(s)
*** TODO Restore file attributes from tree entry
**** TODO File mode
**** TODO File user/group
**** TODO File extended attributes
*** TODO Restore directories from snapshot
**** restoring an empty directory does nothing, should create the directory
**** restore directory mode bits, user/group ownership, extended attributes
*** TODO Detect and prune stale snapshots that never completely uploaded
**** Stale snapshots exist in the database but are not referenced elsewhere
*** TODO Support snapshots consisting only of mode/owner changes
**** i.e. no file content changes, just the database records
** More Better
*** Ransomware protection
**** descriptions of what this means
***** CloudBerry
: CloudBerry Backup detects encryption changes in files and prevents existing
: backups from being overwritten until an administrator confirms if there is an
: issue.
***** Arq:
: Ransomware protection - point-in-time recovery of files
***** https://ruderich.org/simon/notes/append-only-backups-with-restic-and-rclone
: One issue with most backup solutions is that an attacker controlling the local
: system can also wipe its old backups. To prevent this the backup must permit
: append-only backups (also called add-only backups).
****** They change the SSH config to run the backup command with "append only" flag.
*** TODO Permit scheduling upload hours for each day of the week
**** e.g. from 11pm to 6am Mon-Fri, none on Sat/Sun
*** TODO Command-line option to dump database to json (separate by key prefix, e.g. ~chunk~)
*** TODO Support deduplication across multiple computers
**** Place the chunks and packs in a seperate "database" for syncing
***** For RocksDB, use a column family if it helps with =GetUpdatesSince()=
**** RocksDB replication story as of 2019-02-20:
: Q: Does RocksDB support replication?
: A: No, RocksDB does not directly support replication. However, it offers
: some APIs that can be used as building blocks to support replication.
: For instance, GetUpdatesSince() allows developers to iterate though all
: updates since a specific point in time.
***** see =GetUpdatesSince()= and =PutLogData()= functions
**** User configures the host name of the ~peer~ installation
***** Use that to form the URL with which to =sync=
**** Share the chunks and packs documents with a ~peer~ installation
**** At the start of backup, sync with the ~peer~ to get latest chunks/packs
*** TODO Consider how to deal with partial uploads
**** e.g. Minio/S3 has a means of handling these
*** TODO Permit removing a store from a dataset
**** would encourage user to clean up the remote files
**** for local store, could remove the files immediately
**** must invalidate all of the snapshots effected by the missing store
*** TODO Permit moving from one store to another
**** would mean downloading the packs and uploading them to the new store
* Documentation
** Duplicati has a fun description of how the backup works
*** files are broken into "bricks" which go in "bags" and stored in big "boxes" (the pack store)
*** c.f. https://duplicati.readthedocs.io/en/latest/01-introduction/
* Technical Information
** Backup metrics
*** 2023-08-11, 8 cores, 32gb RAM, 4-disk RAID-Z to minio on LAN, 346gb of data
**** backup complete after 9 hours 48 minutes 11 seconds
**** record counts after 1 snapshot
| type   |  count |
|--------+--------|
| chunks |  47751 |
| files  | 134745 |
| packs  |   4133 |
| trees  |  37143 |
| xattrs |      0 |
**** =fileCounts= sans =fileSizes= (which are shown below)
| description |  count |
|-------------+--------|
| total files | 152181 |
| directories |  37356 |
| symlinks    |      0 |
| very small  |   1576 |
| very large  |      3 |
**** =fileSizes=
|      power | count |
|------------+-------|
|         64 |   402 |
|        128 |  2189 |
|        256 |  3431 |
|        512 |  4201 |
|       1024 |  8947 |
|       2048 | 25343 |
|       4096 |  4747 |
|       8192 |  1804 |
|      16384 |  5731 |
|      32768 | 12277 |
|      65536 | 23297 |
|     131072 |  4169 |
|     262144 | 10009 |
|     524288 |  6064 |
|    1048576 |  9046 |
|    2097152 | 23288 |
|    4194304 |  3383 |
|    8388608 |   497 |
|   16777216 |   288 |
|   33554432 |   402 |
|   67108864 |   444 |
|  134217728 |   311 |
|  268435456 |   217 |
|  536870912 |    99 |
| 1073741824 |    17 |
| 2147483648 |     7 |
| 4294967296 |     1 |
** Restore statistics
*** 4 hours to restore 63GB (11k files) of ~tanuki~ data from Google over fiber
** Performance improvements
*** Parallelism
**** more threads means more disk thrashing; SSD is well-suited to this approach
**** shortening snapshot time
***** mini parallelism is 8
***** server parallelism is 4
***** mini before: 555706 files after 3 minutes 2 seconds (original)
***** mini after: 556625 files after 1 minutes 27 seconds (initial)
***** mini after: 571056 files after 1 minutes 23 seconds (subsequent)
***** server before: 147769 files after 1 hours 20 minutes (original)
***** server after: 147769 files after 52 minutes (initial)
***** server after: 148142 files after 52 minutes (subsequent)
*** SHA256 vs BLAKE3
**** server before: 148142 files after 52 minutes (months ago)
**** server after: 152166 files after 42 minutes
** Error Handling
*** what happens to file errors during scanning?
**** any errors during scan result in the entry being completely ignored
**** they will be processed again on the next scan
*** what happens to file errors during packing?
**** if metadata or opening file fails, recorded as having zero length
**** if packing file fails, overall backup will fail
*** what happens when file contents change between scanning and packing?
**** changed file is stored using the original checksum
**** file will subsequently be (needlessly) backed up again next time
** Data Growth
*** main server
**** better pack file generation
***** average pack size ~before~ change: 68,647,434
***** average pack size ~after~ change: 67,688,886
**** original database schema
***** compressed database seems to grow 8mb in 6 months
***** compressed database size: 61,920,768
**** record counts over time
***** as of 2022-03-17
| entity    |  count |
|-----------+--------|
| snapshots |    576 |
| files     | 137081 |
| trees     |  97598 |
| chunks    | 190758 |
| packs     |   4282 |
| xattrs    |  19263 |
***** as of 2023-01-14
| entity    |  count |
|-----------+--------|
| snapshots |    272 |
| files     | 134950 |
| trees     |  65312 |
| chunks    |  56813 |
| packs     |   4107 |
| xattrs    |  18035 |
*** mac mini statistics
**** original database schema
***** 2022-03-15
****** compressed database size: 2,379,181,138
****** database record counts
| entity    |   count |
|-----------+---------|
| snapshots |     190 |
| files     | 1806620 |
| trees     |  302860 |
| chunks    | 1830167 |
| packs     |    3380 |
| xattrs    |  185473 |
**** with file/chunk record optimization
***** 2022-03-18
****** compressed database size: 454,232,580
****** database record counts
| entity    |  count |
|-----------+--------|
| snapshots |      1 |
| files     | 412555 |
| trees     |  48927 |
| chunks    |   4025 |
| packs     |    369 |
| xattrs    |  14388 |
****** only 1% of files are larger than a chunk
**** with new packing algorithm
***** 2022-03-22
****** compressed database size: 462,090,768
****** database record counts
| entity    |  count |
|-----------+--------|
| snapshots |      1 |
| files     | 457980 |
| trees     |  52844 |
| chunks    |   4061 |
| packs     |    176 |
| xattrs    |  13624 |
**** better pack file generation
***** average pack size ~before~ change: 46,960,186
***** average pack size ~after~ change: 70,496,178
**** very small files in database
***** 2022-03-26
****** compressed database size: 463,418,613
****** database record counts
| entity    |  count |
|-----------+--------|
| snapshots |      1 |
| files     | 437775 |
| trees     |  53499 |
| chunks    |   4185 |
| packs     |    188 |
| xattrs    |  12808 |
****** file counts
| type           |  count |
|----------------+--------|
| directories    |  73536 |
| symlinks       |  14543 |
| filesBelow80   |  56407 |
| filesBelow1k   | 293740 |
| filesBelow10k  | 351019 |
| filesBelow100k |  88622 |
| filesBelow1m   |  11461 |
| filesBelow10m  |   2322 |
| filesBelow100m |    222 |
| veryLargeFiles |     16 |
**** working file excludes
***** 2022-03-27
****** compressed database size: 70,466,060
****** database record counts
| entity    |  count |
|-----------+--------|
| snapshots |      1 |
| files     | 321419 |
| trees     |  40786 |
| chunks    |   1533 |
| packs     |     67 |
| xattrs    |    350 |
****** file counts
| type           |  count |
|----------------+--------|
| directories    |  45074 |
| symlinks       |    672 |
| filesBelow80   |  52633 |
| filesBelow1k   | 135193 |
| filesBelow10k  | 311948 |
| filesBelow100k |  65487 |
| filesBelow1m   |   5531 |
| filesBelow10m  |    789 |
| filesBelow100m |    164 |
| veryLargeFiles |      7 |
** Pack files
*** analysis of overly large pack files before accounting for tar entry overhead
**** packing would only account for compressed size of chunks
**** with many small files, tar file overhead increased file size by half (99mb vs 64mb)
| pack digest | count |  file sz | content len | smallest | largest | average |
|-------------+-------+----------+-------------+----------+---------+---------|
| 3fa54d0     | 19193 | 82480128 |    67114835 |       22 | 4755936 |    3496 |
| b93402d     | 39932 | 99137536 |    67109129 |       39 |  446087 |    1680 |
| c57960e     | 38894 | 98344448 |    67111246 |       40 |  452424 |    1725 |
| ef6ff7a     | 40001 | 99184640 |    67111284 |       40 |  492592 |    1677 |
** Possible corner cases
*** Database backup, then restore, then pack prune
Because the database snapshot is recorded in the database after the snapshot
has already been uploaded, if the user were to restore the database and then
perform a pack pruning, the most recent database snapshot would be removed.
