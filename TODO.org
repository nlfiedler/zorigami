* Zorigami
** Phase 2
*** TODO Introduce logging and replace =console= calls with logger
*** TODO Introduce pouchdb and code for reading/writing data
**** TODO Store the ~encryption~ record in PouchDB
*** TODO Introduce GraphQL backend and schema
**** TODO Define the schema
**** TODO Write a simple resolver
**** TODO Write a unit test
*** TODO Decide how snapshots should model changing data and lengthy backups
In other words, while the backup is occuring, files are changing. The
snapshot will inevitably not record these changes. As such, a snapshot
really represents different files at different times. For instance, file A
was backed up a time T1, and file B was saved later at time T2, but all in
snapshot S1. The finest granularity is going to be the packfile, so record
the time the files in that pack were saved as the time that the pack was
created.
**** During the backup, tree objects will have a ~working~ state
**** Once a tree is updated with the final results, it is marked as ~complete~
**** As such, the tree objects may be change during the backup

** Phase 3
*** TODO Write a ReasonML frontend
**** TODO Add =bs-platform= dependency and =bsconfig.json= file
**** TODO Put front-end code in a directory named =web-src=
**** TODO Set up =gulp= and =webpack= to build the front-end code
**** TODO Set up the routing
**** TODO Write a simple home page that shows something
*** TODO Update the architecture and data model in =NOTES.md=
** Phase 4
*** TODO Write the logic for the "engine"
**** TODO Produce a ~working~ snapshot of the dataset
**** TODO Detect symbolic links and ignore them
- Use =fs.lstat()= and =stats.isSymbolicLink()= to detect symlinks
**** TODO Compute the difference from the previous snapshot
**** TODO Produce and upload pack files containing new/changed files
***** Splitting large files across packs
*** TODO Consider how to handle small changes to large files
**** e.g. using merkle tree of "bytes" to save only changed parts of large files
**** c.f. perkeep.org design/code for an example of dealing with large files
- uses Merkle tree of "bytes" schema blobs to represent large files
- data stored at the leaves of the tree
- rolling checksum cut points (ala rsync, bup)
- de-duplication within files and shifting files
- efficient seeks/pread
*** TODO Store database in a bucket named after the "computer UUID"
*** TODO File restoration should apply user/group at time from which file is being restored
*** TODO Store pack files in Google Cloud Storage
- https://github.com/googleapis/nodejs-storage/
** Phase 5
*** TODO Support snapshots consisting only of mode/owner changes
**** i.e. no file content changes, just the database records
*** TODO Store pack files in Amazon Glacier
**** c.f. https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/welcome.html
**** Offer user option to use "expedited" retrievals so they go faster

* Electron App
** Phase N
*** Create a system tray icon/widget
**** Popup menu like Time Machine
**** Show current status, last backup
**** Action to open the app and examine snapshots
**** Action to open the app and check settings
