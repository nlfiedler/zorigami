* Zorigami
** kohaku local backup
*** DONE form for defining a local store
*** DONE form for configuring dataset
**** DONE on home page, indicate if there are no stores defined, link to stores page
**** DONE add forms for adding/editing datasets to the home page
***** use the same basic logic as the stores page
***** DONE validate the schedule form inputs
***** DONE allow schedule to be blank with default
***** DONE allow packsize to be blank with default
***** DONE validate the pack size form inputs
***** DONE convert the pack_size value from MB to bytes on new/update
***** DONE pack size input is not populated in edit form
**** DONE select a single store for the dataset (just one for now)
**** DONE set the backup schedule with simple rules (e.g. "daily")
***** generate cron-like schedules based on checkboxes for datasets
****** e.g. "daily" produces =@daily=, "hourly" produces =@hourly=
**** DONE dataset delete is failing
*** DONE currently when running locally the backup thread panics
**** DONE remove all of the =unwrap()= calls, replace with proper error handling
**** DONE add more logging in the error cases
*** DONE ensure backup progress is logged to the console
*** DONE find all =panic!= calls and ensure there is error logging as well
*** DONE replace =gpgme= with =libsodium= which seems better
**** unclear if gpgme is available on windows
**** would need to distribute gpgme with our application
**** c.f. https://download.libsodium.org/doc/
**** rust crate: https://crates.io/crates/sodiumoxide
***** appears to pull down the library and compile/link statically
**** DONE comment out the encrypt/decrypt functions
**** DONE remove the gpgme dependencies from =Cargo.toml=
**** DONE remove the gpgme installs in =Dockerfile=
**** DONE add =sodiumoxide= dependency to =Cargo.toml=
**** DONE read documentation: https://docs.rs/sodiumoxide/0.2.4/sodiumoxide/
***** DONE how to create an encryption key from a passphrase
****** look for =crypto_pwhash()=
***** DONE how to encrypt/decrypt a stream/file
**** DONE write new encrypt/decrypt functions using sodium
***** DONE use the =ONCE= thing to invoke =sodiumoxide.init()= once
***** DONE produce a key from the given passphrase and a random salt for each pack file
***** DONE encrypt the pack file using the derived key
***** DONE store the salt for each pack in the pack record in the database
***** DONE decrypt the pack file using the derived key
***** DONE read the salt from the corresponding pack record
**** DONE update all references to ~openpgp~ everywhere
*** DONE fix the clippy issues found in =sftp.rs=
*** DONE catch and report file errors in =process_path()= and gracefully skip that entry
*** DONE seems like multiple jobs are spawning at the same time
**** kernel errors in the =dmesg= output
#+BEGIN_SRC
[230312.758919] INFO: task zorigami:9494 blocked for more than 120 seconds.
[230312.758970]       Tainted: P           O     4.15.0-65-generic #74-Ubuntu
[230312.758986] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
[230312.759004] zorigami        D    0  9494   7537 0x00000120
[230312.759006] Call Trace:
[230312.759010]  __schedule+0x24e/0x880
[230312.759011]  ? mutex_lock+0x12/0x40
[230312.759012]  schedule+0x2c/0x80
[230312.759017]  cv_wait_common+0x11e/0x140 [spl]
[230312.759019]  ? wait_woken+0x80/0x80
[230312.759022]  __cv_wait+0x15/0x20 [spl]
[230312.759051]  dmu_buf_hold_array_by_dnode+0x1c6/0x450 [zfs]
[230312.759066]  dmu_read_uio_dnode+0x49/0x100 [zfs]
[230312.759079]  dmu_read_uio_dbuf+0x49/0x70 [zfs]
[230312.759101]  zfs_read+0x136/0x3f0 [zfs]
[230312.759124]  zpl_read_common_iovec+0x80/0xd0 [zfs]
[230312.759145]  zpl_iter_read+0x9c/0xe0 [zfs]
[230312.759147]  new_sync_read+0xe4/0x130
[230312.759148]  __vfs_read+0x29/0x40
[230312.759149]  vfs_read+0x8e/0x130
[230312.759150]  SyS_read+0x5c/0xe0
[230312.759152]  do_syscall_64+0x73/0x130
[230312.759154]  entry_SYSCALL_64_after_hwframe+0x3d/0xa2
[230312.759155] RIP: 0033:0x7f78a35b8544
[230312.759156] RSP: 002b:00007f78913bcb40 EFLAGS: 00000246 ORIG_RAX: 0000000000000000
[230312.759157] RAX: ffffffffffffffda RBX: 00000000000000b0 RCX: 00007f78a35b8544
[230312.759157] RDX: 0000000000002000 RSI: 00007f78913bcbe8 RDI: 00000000000000b0
[230312.759158] RBP: 00007f78913bcbe8 R08: 0000000000000000 R09: 000000004e7dd2ac
[230312.759158] R10: 00000000f3f61b17 R11: 0000000000000246 R12: 0000000000002000
[230312.759159] R13: 00007f78913bcb90 R14: 0000000000080000 R15: 00007f78913becf0
#+END_SRC
**** many zorigami processes are spawned
**** lot of CPU activity
**** no pack files were made
**** really long log file
**** seems like the supervisor is starting the backup repeatedly
*** DONE log snapshot metrics after =take_snapshot()= call
**** number of files
**** time duration
*** TODO use this to replace =replicaz= for local backup
**** build the image locally, push to docker registry
**** deploy as a single instance, using volumes for data
**** DONE use a multi-stage build to minimize the final image size
***** build the application in a "build" container
***** then start another "final" container and copy over the binary
**** DONE define a volume for the database
**** DONE define a volume for the local pack store
**** DONE define a volume for the datasets
**** DONE use =rust:latest= for the build container
**** DONE use [[https://github.com/mastertinner/healthcheck][mastertinner/healthcheck]] as a healthcheck (written in Rust)
***** copy the code since it appears to be a dead project
***** set up a build container just for this, copy to final image
**** DONE install gpgme, build tools
**** DONE run =cargo build --release= to build
**** DONE define a new final container based on =debian:latest=
***** a.k.a. "buster" on which rust:latest is based
**** DONE install gpgme package
**** DONE set a working directory (=WORKDIR=)
**** DONE copy the build artifacts from "builder" image to workdir
***** use =COPY --from= to copy from a particular image
**** DONE expose port =8080=
**** DONE =ENTRYPOINT= will be =RUST_LOG=info ./target/release/zorigami=
**** DONE run as a non-root user
**** DONE the process crashes soon after starting backup
***** it was likely due to the old docker-machine and/or docker-machine-nfs
***** works fine with Docker for Mac and Docker Engine on Linux
**** DONE push the final image to the registry
***** c.f. https://docs.docker.com/registry/
#+BEGIN_SRC shell
$ docker image tag zorigami_app 192.168.1.3:5000/zorigami_app
$ docker push 192.168.1.3:5000/zorigami_app
#+END_SRC
**** DONE test on =yubaba= before deploying to =kohaku=
**** DONE pack files are all zero bytes: gpgme is not working in docker
***** the gpg encryption is producing zero byte files
***** shared object dependencies are all okay (it would not start otherwise)
***** try running the container as privileged: no good
***** works correctly _outside_ of docker
***** should log the actual error in =encrypt_file()=
***** maybe the =PATH= does not include =/usr/bin= in the container
: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
***** maybe =/usr/bin/gpg= needs to be installed (=gpg= package): libgpgme11 already depends on it
***** environment within the container:
#+BEGIN_SRC shell
chidori@6d613a85009d:/zorigami$ env | sort
DEBIAN_FRONTEND=noninteractive
HOME=/home/chidori
HOST=0.0.0.0
HOSTNAME=6d613a85009d
LS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40...
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
PORT=8080
PWD=/zorigami
RUST_LOG=info
SHLVL=0
TERM=xterm
_=/usr/bin/env
#+END_SRC
***** my environment on the docker host
#+BEGIN_SRC shell
DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/1000/bus
HOME=/home/nfiedler
LANG=en_US.UTF-8
LC_TERMINAL=iTerm2
LC_TERMINAL_VERSION=3.3.4
LESSCLOSE=/usr/bin/lesspipe %s %s
LESSOPEN=| /usr/bin/lesspipe %s
LOGNAME=nfiedler
LS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40...
MAIL=/var/mail/nfiedler
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/nfiedler/.cargo/bin
PWD=/home/nfiedler
SHELL=/bin/bash
SHLVL=1
SSH_CLIENT=192.168.1.65 50940 22
SSH_CONNECTION=192.168.1.65 50940 192.168.1.3 22
SSH_TTY=/dev/pts/0
TERM=xterm-256color
USER=nfiedler
_=/usr/bin/env
XDG_DATA_DIRS=/usr/local/share:/usr/share:/var/lib/snapd/desktop
XDG_RUNTIME_DIR=/run/user/1000
XDG_SESSION_ID=760
#+END_SRC
***** maybe need to set =USER= so that gpg can write files: did not work
***** maybe something about the passphrase provider is not working: it is not invoked
**** DONE create a host user to own the files on the host system
**** DONE try building without copying the =libzorigami.rlib= file
**** DONE document the build and deploy procedure in =README.md=
**** DONE define volumes and environment variables for production
***** =DB_PATH=
***** =PASSPHRASE=
**** DONE stop the =replicaz_safe_1= container
**** DONE destroy the =safe/shared= zfs dataset
**** DONE create the =safe/packs= zfs dataset
** restore file via webui
*** TODO move datasets to their own page
*** TODO add a navbar link for the datasets route
*** TODO browse snapshots via webui
*** TODO browse directory tree via webui
*** TODO tree entries of =ERROR= type should be displayed as such
*** TODO restore a file to local disk via webui
** kohaku remote (SFTP) backup
*** TODO should maybe have an initial backup delay on startup
**** it starts up and immediately starts processing backups
*** TODO form for defining an SFTP store
**** TODO new store form needs to have a "kind" selector
**** TODO selecting different kind will rebuild new store form accordingly
**** TODO make a different form and validation module for each type of store
*** TODO support SFTP with private key authentication
*** TODO allow private key that is locked with a passphrase
*** TODO use this to replace =replicaz= for remote backup
** Loose issues
*** TODO sometimes creating a new store results in a network error
*** TODO sometimes =test_db_threads_one_path()= test fails getting a lock
**** seemingly only on Ubuntu (maybe Debian, need to test)
** Loose GraphQL tasks
*** TODO test the GraphQL schema and resolvers
**** TODO "integers" that are not radix 10 integers
**** TODO digests that lack the proper algorithm prefix
**** TODO querying for things when there is nothing in the database
**** TODO querying snapshots
**** TODO querying trees
**** TODO querying files
**** DONE fetching configuration record
**** TODO updating configuration record
**** DONE querying datasets
**** DONE mutating datasets
**** DONE querying stores
**** DONE mutating stores
*** TODO find out how to document arguments to mutations
*** TODO handle errors in getting Database ref in graphql handler
** Loose WebUI tasks
*** TODO Update to the latest release of reductive
*** TODO improve the page for defining datasets
**** TODO for now, =basepath= and =stores= must be write-once
***** changing them would screw up everything
**** TODO store selection should be easier for the user
**** TODO schedule input should be easier to use
***** user should not have to type ~@daily~ literally
**** TODO disable Save button until form is valid
**** TODO store input validator should check stores actually exist
**** TODO pack size should have minimum and maximum values
*** TODO improve the page for defining stores
**** TODO disable Save button until form is valid
**** TODO remove the edit/cancel button, form is always in edit mode with save button
**** TODO delete button should be far away from the other button(s)
**** TODO delete button should require two clicks, with "are you sure?"
**** TODO Display help text on stores page when there are no stores defined
**** TODO Display help text on home page when there are no datasets defined
**** TODO Scroll to form when edit button is clicked
***** with a bunch of stores on the screen, click ~Edit~ for last one
***** page refreshes and scrolls to the top
**** TODO Autofocus input field on edit
***** this is tricky with React, =autofocus= is not really honored
***** can do it if we turn the input element into a full-fledged component
***** and use the =useRef()= hook to set the focus on the HTML element
***** c.f. https://reactjs.org/docs/hooks-reference.html#useref
** Robustness and Recovery
*** TODO =core::scan_tree()= should not return a Result, deal with all of the errors
**** need to be robust against all sorts of weird file access issues
*** TODO store database in a bucket named after the "computer UUID"
**** pack file should be a ULID so that the most recent entry is sorted last
**** glacier will use s3 to track the "compuuter UUID" to vault name mapping
*** TODO detect files changing between snapshot and pack building time
**** use the =changed= record property to track this
*** TODO detect file deletion during backup, mark file record as skipped
**** Basically handle the error and mark the record as "failed"
*** TODO verify checksum of downloaded packs during restore
*** TODO consider using [[https://github.com/vertexclique/bastion][bastion]] for fault-tolerance (i.e. supervisor)
**** it appears to support Erlang-style supervisor behavior
*** TODO recover from a backup thread that panicked
**** For each spawned backup thread, spawn a supervisor thread
**** Supervisor thread joins the backup thread
**** If the =Result= from =JoinHandle.join()= is =Err=, then restart
**** see also example on docs for =std::thread::panicking()=
*** TODO how to recover from the main supervisor thread panicking?
**** Perhaps rely on cron, launchd, etc to keep things running
*** TODO handle termination signals to exit even if backup is running
**** leave the cleanup process for next time
**** =actix_server= already handles =SIGINT= and =SIGTERM=
*** TODO maybe use thread pools and futures in supervisor
**** futures would help with reporting errors back to the main thread
*** TODO support database integrity checks
**** ensure all referenced records actually exist
**** like git fsck, start at the top and traverse everything
**** find and report dangling objects
**** an automated scan could be run on occasion
** Google Backup
*** TODO support excluding certain file patterns from backup
*** TODO add store that supports Google Cloud Storage
**** Check for bucket name collisions and retry in pack store
**** https://cloud.google.com/storage/docs/best-practices
*** TODO form for defining a Google Cloud Storage store
*** TODO support scheduling upload times, like akashita does
**** Define a set of hours each day when uploads should occur
**** Can make use of [[https://crates.io/crates/chrono][chrono]] crate for time related operations
*** TODO use this to replace =akashita= for online backups
** More Functionality
*** TODO event dispatching for the web and desktop
**** use the state management to manage "events" and state
**** engine emits actions/events to the store
***** for backup and restore functions
***** e.g. "downloaded a pack", "uploaded a pack"
**** store holds the cumulative data so late attachers can gather everything
**** supervisor threads register as subscribers to the store
**** clients will use GraphQL subscriptions to receive updates
**** supervisor threads emit GraphQL subscription events
*** TODO exclude the database files from the dataset(s)
*** TODO allow assigning multiple stores to a given dataset
*** TODO consider how datasets can be modified after creation
**** should their stores be allowed to change?
**** should their basepath be allowed to change?
**** cannot change stores assigned to dataset once there are snapshots
** Architecture Review
*** Embedded Database
**** Is the default RocksDB performance sufficient?
**** Consider https://github.com/spacejam/sled/
***** written in Rust, open source
***** will need prefix key scanning
****** looks like you just use a prefix of the key (sorts before the matching keys)
*** Client/Server
**** Look at ways to secure the server, to allay fears of exploits
**** A web conferencing tool was exploited via its hidden HTTP server
** macOS support
*** TODO Use =launchd= to manage the process, have it start automatically
*** TODO Use this to replace Time Machine (store on server using SFTP)
** Full Restore
*** TODO Restore file attributes from tree entry
**** TODO File mode
**** TODO File user/group
**** TODO File extended attributes
*** TODO Restore directories from snapshot
**** TODO Directory mode
**** TODO Directory user/group
**** TODO Directory extended attributes
**** TODO Restore multiple files efficiently
**** TODO Restore a directory tree efficiently
*** TODO Detect and prune stale snapshots that never completely uploaded
**** Stale snapshots exist in the database but are not referenced elsewhere
*** TODO Support snapshots consisting only of mode/owner changes
**** i.e. no file content changes, just the database records
*** TODO Restore the backup database
**** TODO Restore to a different directory, then copy over records
** Windows support
*** TODO Try building on Windows
*** TODO Support Windows file types
**** ReadOnly
**** Hidden
**** System
** More Better
*** TODO Automatically prune backups more then N days old
**** For Google and Amazon, anything older than 90 days is free to remove
**** This would be a configuration setting, with defaults and path-specific
*** TODO Option to keep N daily, M weekly, and P monthly backups (a la Attic backup)
*** TODO Permit scheduling upload hours for each day of the week
**** e.g. from 11pm to 6am Mon-Fri, none on Sat/Sun
*** TODO Command-line option to dump database to json (separate by key prefix, e.g. ~chunk~)
*** TODO Ability to pause or cancel a backup
*** TODO Support deduplication across multiple computers
**** Place the chunks and packs in a seperate "database" for syncing
***** For RocksDB, use a column family if it helps with =GetUpdatesSince()=
**** RocksDB replication story as of 2019-02-20:
: Q: Does RocksDB support replication?
: A: No, RocksDB does not directly support replication. However, it offers
: some APIs that can be used as building blocks to support replication.
: For instance, GetUpdatesSince() allows developers to iterate though all
: updates since a specific point in time.
***** see =GetUpdatesSince()= and =PutLogData()= functions
**** User configures the host name of the ~peer~ installation
***** Use that to form the URL with which to =sync=
**** Share the chunks and packs documents with a ~peer~ installation
**** At the start of backup, sync with the ~peer~ to get latest chunks/packs
*** TODO Consider how to deal with partial uploads
**** e.g. Minio/S3 has a means of handling these
*** TODO Design garbage collection solution (see NOTES)
*** TODO Pack store should recommend pack sizes
**** e.g. Glacier recommends archives greater than 100mb
**** can only really make a recommendation, the user has to choose the right size
*** TODO Permit removing a store from a dataset
**** would encourage user to clean up the remote files
**** for local store, could remove the files immediately
**** must invalidate all of the snapshots effected by the missing store
*** TODO Permit moving from one store to another
**** would mean downloading the packs and uploading them to the new store
*** TODO Support Amazon S3, Minio
**** Need to limit number of remote buckets to 100
**** Bucket limit: catch the error and handle by re-using another bucket
*** TODO Support Amazon Glacier
**** Need to limit number of remote buckets to 1000
**** Use S3 to store the database-to-archive mapping of each snapshot
**** Offer user option to use "expedited" retrievals so they go faster
*** TODO Support Amazon Cloud Drive
*** TODO Support Microsoft Azure blob storage
*** TODO Support Backblaze B2
*** TODO Support [[https://wiki.openstack.org/wiki/Swift][OpenStack Swift]]
*** TODO Support Wasabi
*** TODO Support Google Drive
*** TODO Support Google Cloud Coldline
*** TODO Support Dropbox
*** TODO Support Oracle Cloud Storage
*** TODO Support IBM Cloud Storage
*** TODO Support Rackspace Cloud Files
*** TODO Consider how to backup and restore FIFO, BLK, and CHR "files"
**** c.f. https://github.com/jborg/attic/blob/master/attic/archive.py
**** c.f. https://github.com/avz/node-mkfifo (for FIFO)
**** c.f. https://github.com/mafintosh/mknod (for BLK and CHR)
* Product
** TODO Evaluate other backup software
*** TODO Check out some on App Store
**** Backup Guru LE
**** ChronoSync Express
**** Backup
**** Remote Backup Magic
**** Sync - Backup and Restore
**** Backup for Dropbox
**** Freeze - for Amazon Glacier
*** Lot of "folder sync" apps out there
** Define the target audience
*** Average home user, no technical expertise required
** Need distinquishing features
*** TODO What sets this application apart from the other polished products?
**** Cross-platform (e.g. macOS, Windows)
**** Linux server ready
** Packaging
*** Need to bundle the gpgme library
** Windows Certified
*** CloudBerry(?) has bunches of certifications
*** is that really so meaningful? *I* never cared
** Name
*** Joseph suggests "Attic"
**** =atticapp.com= is taken
**** =attic.app= is for sale
**** Look for ~attic~ in different languages
**** Esperanto: ~mansardo~
***** also means something in Macedonian
**** Hawaiian: ~kaukau~
**** Latin: ~atticae~
* Technical Information
** Exploring other languages
*** Compile to native for easy deployment
*** Compile to native for code obfuscation
*** Rust
**** Advantages
***** compile to native
***** expressive, safe type system
***** good dependency management
***** lots of useful tools (e.g. clippy)
**** Disadvantages
***** fewer libraries compared to Go
**** DONE GraphQL server
***** Make sure it can generate a schema.json
***** Should be able to parse schema definition (for docs)
***** https://github.com/graphql-rust/juniper (BSD)
****** supports entire GraphQL specification
****** does /not/ read GraphQL schema language
****** supports GraphiQL and Playground
****** is not the HTTP server, but integrates with them
****** uses macros for schema documentation
***** tutorial at [[http://alex.amiran.it/post/2018-08-16-rust-graphql-webserver-with-warp-juniper-and-mongodb.html][alex.amiran.it]] that uses warp web framework
***** old https://github.com/nrc/graphql (MIT/Apache)
**** DONE Web framework
***** our needs are simple, so a simple framework is best
***** Actix https://actix.rs (Apache 2.0)
****** works with stable Rust
****** powerful and easy to use
****** testing library
****** integrates with juniper
****** offers state management for web code
****** lot more actively used than warp
***** warp https://github.com/seanmonstar/warp (MIT)
****** works with stable Rust
****** powerful and easy to use
****** testing library
****** integrates with juniper
***** Rocket https://rocket.rs (Apache 2.0)
****** requires Rust nightly because of fancy macros
****** routing using macros
****** streams input and output
****** cookies
****** json
****** environment configuration
****** testing library
****** integrates with juniper
***** Gotham https://gotham.rs (MIT/Apache 2.0)
****** targets stable Rust
****** routing
****** middleware
****** sessions
****** cookies
****** templates
****** testing library
****** how to integrate with juniper is unknown
***** Iron http://ironframework.io (MIT)
****** crate has not been updated since 2017
****** everything is middleware that must be added in
****** integrates with juniper
***** Nickel http://nickel-org.github.io (Express.js like) (MIT)
****** pretty basic compared to Rocket
***** pretty basic https://github.com/carllerche/tower-web (MIT)
****** competing with warp? hyper?
**** DONE Database
***** ideally want something well maintained, reliable
***** schema is pretty simple, could use key/value store
***** RocksDB https://github.com/rust-rocksdb/rust-rocksdb (Apache)
****** statically links everything, including compression support
***** SQLite https://github.com/jgallagher/rusqlite (MIT)
***** Rust wrapper to LevelDB https://github.com/skade/leveldb
***** LevelDB in Rust (active?) https://bitbucket.org/dermesser/leveldb-rs/overview
**** DONE dotenv
***** https://github.com/dotenv-rs/dotenv (MIT)
**** DONE Configuration
***** https://github.com/mehcode/config-rs (MIT/Apache)
**** DONE =getpwuid= and =getgrgid= support
***** libc: https://crates.io/crates/libc (MIT/Apache 2.0)
**** DONE test library
***** https://github.com/rust-rspec/rspec (MPL-2.0)
****** appears to be dead
***** https://github.com/utkarshkukreti/speculate.rs (MIT)
****** works well for integration tests
**** DONE UUID support
***** https://github.com/uuid-rs/uuid (MIT/Apache 2.0)
**** DONE xattr support
***** Unix only: https://github.com/Stebalien/xattr (MIT/Apache 2.0)
**** DONE CDC
***** https://github.com/jrobhoward/quickcdc (MIT/Apache 2.0)
****** not quite FastCDC, given dates of paper, but should be close enough
****** use a constant salt value for predictable results
****** example uses =memmap= crate to read large files
**** DONE Tar file
***** https://github.com/alexcrichton/tar-rs (MIT/Apache 2.0)
**** DONE PGP/Encryption
***** https://github.com/gpg-rs/gpgme (LGPL)
****** will need to bundle the =gpgme= library (unless statically linked)
***** cryptostream https://github.com/neosmart/cryptostream (MIT)
***** basic packets [[https://github.com/csssuf/pretty-good][csssuf/pretty-good]]
***** read only [[https://nest.pijul.com/pmeunier/openpgp][pijul]] openpgp
**** DONE ULID
***** https://github.com/dylanhart/ulid-rs (MIT)
**** DONE SFTP client
***** https://github.com/alexcrichton/ssh2-rs (MIT/Apache 2.0)
**** DONE AWS client
***** Rusoto https://www.rusoto.org (MIT)
**** DONE Google Cloud client
***** https://github.com/Byron/google-apis-rs (MIT/Apache 2.0)
**** DONE Minio client
***** Rusoto supports Minio https://github.com/rusoto/rusoto (MIT)
*** Go vs Rust
**** Go: first class support for cloud services
**** Go: statically linked OpenPGP readily available
**** Go: easy to read and write language
**** Rust: mature dependency management tooling
**** Rust: cargo has good editor support
**** Rust: expressive type system
**** Rust: nominal subtyping is much easier to follow
**** Rust: streamlined error handling
**** Rust: fine-grained namespaces and visibility control
