* Zorigami
** Phase 2
*** DONE Find out average file size on macOS
**** For ~nfiedler~ home directory:
***** 330,592 total files
***** 330,213 files under 4 MB
***** average file size: 198 KB
***** files 100 MB to 1 GB: 11
***** files 1 GB to 10 GB: 4
***** 3 largest files: 14, 14, 16 GB
**** For ~p4nathan~ home directory:
#+BEGIN_SRC shell
$ gfind . -type f -fprintf /tmp/sizes '%k\n'

$ awk '{ total += $1; count++ } END { print total/count }' /tmp/sizes
580.741

$ sort -n /tmp/sizes | uniq -c
$ awk '{ total += $1 } END { print total }' /tmp/4kfiles.txt
#+END_SRC
***** 145,051 total files
***** 144,768 files under 4 MB
***** average file size: 580 KB
***** files 100 MB to 1 GB: 3
***** files 1 GB to 10 GB: 9
***** 3 largest files: 10, 17, 18 GB
*** DONE Find out average file sizes in the shared data set
**** =/zeniba/shared=
***** 124,381 total files
***** 122,354 files under 4 MB
***** average file size: 2473 KB
***** files 100 MB to 1 GB: 529
***** files 1 GB to 10 GB: 16
***** 3 largest files: 6, 6, 34 GB
*** DONE Find out file sizes for LevelDB
**** logs grow to 4 MB
**** db files grow to 2 MB
*** DONE Consider if PouchDB/LevelDB will be adequate for tracking 300,000 files
**** DONE Try testing with some simple live data test
***** 334,147 files in total
***** 230,219 unique files yields 73 MB of LevelDB data
***** runs in a 8 minutes
*** DONE Write the logic for the "engine"
**** DONE Store the ~encryption~ record in PouchDB
**** DONE Need a strategy for scanning file tree without blowing up
**** DONE Use =bin/dbfiles.js= as a guide for the above ~tree walker~
***** DONE Use =async= and =await= in a single function to process tree
***** DONE Need to return both files and directories
**** DONE Move the =dbfiles.ts= into =engine.ts= and start testing
**** DONE Compute the difference from the previous snapshot
**** DONE Produce a ~working~ snapshot of the dataset
**** DONE Get the user and group names in addition to uid, gid
***** look for =getpwuid= and =getgrgid= support
***** https://github.com/prantlf/node-posix-ext
**** DONE Get the xattrs and store in database (deduplicate)
**** DONE Detect symbolic links and store their reference
**** DONE Produce and upload pack files containing new/changed files
**** DONE Remove chunks already in database before building pack file
*** DONE Need a reliable means of deleting the test database every time
*** DONE Restore files from snapshot
**** DONE File content
*** DONE Use [[https://www.minio.io/][Minio]] to develop another remote store implementation
**** c.f. https://docs.minio.io/docs/javascript-client-quickstart-guide.html
**** DONE Install the =minio= npm
**** DONE Develop the minio store implementation
**** DONE Test using docker container
*** TODO Consider if =findChangedFiles()= should be an async generator
*** DONE Store needs to support deleting objects and buckets
*** DONE Consider how to implement "transactions" to recover from failed operations
** Phase 3
*** TODO Use starter [[https://github.com/Microsoft/TypeScript-Node-Starter][guide]] to get Node set up with TypeScript
***** TODO Look more at how https://github.com/TypeStrong/ts-node can be used
***** TODO Is rewriting =app.js= worthwhile or necessary?
***** TODO Translate the routes
*** TODO Maybe rewrite =gulpfile.ts= in TypeScript
***** c.f. https://github.com/TypeStrong/ts-node
***** https://github.com/vvakame/typescript-project-sample/blob/master/gulpfile.ts
*** TODO Introduce GraphQL backend and schema
**** TODO Define the schema
**** TODO Write a simple resolver
**** TODO Write a unit test
*** TODO Write a ReasonML frontend
**** TODO Add =bs-platform= dependency and =bsconfig.json= file
**** TODO Put front-end code in a directory named =web-src=
**** TODO Set up =gulp= and =webpack= to build the front-end code
**** TODO Set up the routing
**** TODO Write a simple home page that shows something
*** TODO Recover from unfinished backup procedure
**** When performing backup, check if latest snapshot exists and lacks an end time
**** If so, try to continue the backup with that snapshot
**** If not, make a new snapshot
**** If there are no changes at all (tree or file), delete the snapshot
*** TODO Manage datasets, defaults, saving updated snapshot to database
*** TODO Exclude the database files from the data set(s)
** Phase 4
*** TODO Use this to replace =replicaz= by persisting to USB drive
*** TODO Use this to replace =replicaz= by persisting over SFTP
*** TODO Support multiple roots per dataset
*** TODO Verify checksum of downloaded packs during restore
*** TODO Store database in a bucket named after the "computer UUID"
*** TODO Store pack files in Google Cloud Storage
**** c.f. https://github.com/googleapis/nodejs-storage/
*** TODO Use this to replace =akashita= for online backups
** Phase 5
*** TODO Restore file attributes from tree entry
**** TODO File mode
**** TODO File user/group
**** TODO File extended attributes
*** TODO Restore directories from snapshot
**** TODO Directory mode
**** TODO Directory user/group
**** TODO Directory extended attributes
**** TODO Restore multiple files efficiently
**** TODO Restore a directory tree efficiently
*** TODO Detect file deletion during backup, mark file record as skipped
*** TODO Support snapshots consisting only of mode/owner changes
**** i.e. no file content changes, just the database records
*** TODO Restore the backup database
**** TODO Restore to a different directory, then copy over records
*** TODO Support deduplication across multiple computers
**** Place the chunks and packs in a seperate "database" for syncing
**** Use the express support in [[https://github.com/pouchdb/pouchdb-server][pouchdb-server]] to serve up chunks/packs db
**** User configures the host name of the ~peer~ installation
***** Use that to form the URL with which to =sync=
**** Share the chunks and packs documents with a ~peer~ installation
**** At the start of backup, sync with the ~peer~ to get latest chunks/packs
*** TODO Automatically prune backups more then N days old
**** For Google and Amazon, anything older than 90 days is free to remove
**** This would be a configuration setting, with defaults and path-specific
*** TODO Option to keep N daily, M weekly, and P monthly backups (a la Attic backup)
** Phase 6
*** TODO Consider how to deal with partial uploads (e.g. Minio/S3 has a means of handling these)
*** TODO =tarsnap= author suggests compression is vulnerable to exploit
: To protect against an attacker who has a zlib exploit and can tamper with
: our backups, we append a "physical" HMAC to the end of each block.
*** TODO Support Windows file types
**** ReadOnly
**** Hidden
**** System
*** TODO Support Amazon S3
*** TODO Support Amazon Glacier
**** c.f. https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/welcome.html
**** Offer user option to use "expedited" retrievals so they go faster
*** TODO Support Microsoft Azure blob storage
*** TODO Support Backblaze B2
*** TODO Support [[https://wiki.openstack.org/wiki/Swift][OpenStack Swift]]
*** TODO Support Wasabi
*** TODO Support Google Drive
*** TODO Support Dropbox
*** TODO Support Oracle Cloud Storage
*** TODO Support IBM Cloud Storage
*** TODO Support Rackspace Cloud Files
*** TODO Consider how to backup and restore FIFO, BLK, and CHR "files"
**** c.f. https://github.com/jborg/attic/blob/master/attic/archive.py
**** c.f. https://github.com/avz/node-mkfifo (for FIFO)
**** c.f. https://github.com/mafintosh/mknod (for BLK and CHR)
* Electron App
** Phase N
*** TODO Write it in TypeScript
*** TODO Create a system tray icon/widget
**** Popup menu like Time Machine
**** Show current status, last backup
**** Action to open the app and examine snapshots
**** Action to open the app and check settings
* Product
** Name
*** Joseph suggests "Attic"
**** =atticapp.com= is taken
**** =attic.app= is for sale
**** Look for ~attic~ in different languages
**** Esperanto: ~mansardo~
***** also means something in Macedonian
**** Hawaiian: ~kaukau~
**** Latin: ~atticae~
