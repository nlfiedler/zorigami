* Tasks
** crate updates
*** latest =time= crate is incompatible with =chrono= for now
** Clean Architecture
*** Use dependency injection to connect parts together
**** https://crates.io/crates/shaku (compile-time IoC)
**** https://crates.io/crates/coi (compile-time IoC)
**** consider if writing one from scratch (a la =get_it=) would be adequate
*** Organize the code into domain/data/preso layers
**** Domain
***** entities
****** checksum
****** chunk
****** dataset
****** file
****** pack
****** schedule
****** snapshot
****** tree
****** tree entry
****** tree entry type
****** tree reference
***** managers
****** =state.rs= to manage state
****** =backup.rs= (nee =engine.rs=) to perform backup operation
****** =restore.rs= (nee =engine.rs=) to perform restore operation
****** =process.rs= (nee =supervisor.rs=) to manage scheduled backups
***** repositories
****** record database (CRUD for the entities)
****** pack store (CRUD for the pack files)
***** use cases
****** define data set
****** define pack store
****** delete data set
****** delete pack store
****** get data sets
****** get pack stores
****** get snapshot
****** get tree
****** update data set
****** update pack store
****** start backup
****** stop backup
****** restore file
**** Data
***** pack store implementations become data sources for packs
***** pack repository would provide logic for handling multiple sources
****** uses store type to select data source implementation
****** =store_pack()=
****** =retrieve_pack()=
**** Presentation
***** the =#[graphql]= on domain entities will need to be implemented manually in preso layer
***** basically the graphql schema implementation
***** probably do not need =dataset()= and =store()= queries
***** can remove the =dataset()= and =store()= queries from Dart code, they are unused
*** TODO Action Plan
**** DONE implement ~new store~ usecase
***** define =Store= entity
***** define =StoreDef= model for serialization
**** DONE copy enough =schema.rs= to preso layer to expose ~new store~ mutation
***** change GraphQL representation of =Store= to match model
****** id: String!
****** type: String!
****** label: String!
****** properties: [Property!]! (where Property is a name/value string pair)
**** DONE enable enough =main.rs= to expose the GraphQL endpoint
**** DONE implement ~get stores~ usecase (and graphql query)
**** DONE implement ~update store~ usecase (and graphql mutation)
**** DONE implement ~delete store~ usecase (and graphql mutation)
**** DONE implement ~new data set~ usecase (and graphql mutation)
**** DONE implement ~get data sets~ usecase (and graphql query)
**** DONE implement ~update data set~ usecase (and graphql mutation)
***** DONE remove =computer_id= from =Dataset=, store as a separate record
***** DONE remove =latest_snapshot= from =Dataset=, store as a separate record
***** DONE add repo functions to put/get computer id for a dataset
***** DONE add repo functions to put/get latest snapshot for a dataset
***** DONE store computer_id as =computer/<dataset.key>= with raw string value
***** DONE store latest snapshot as =latest/<dataset.key>= with raw string value of checksum
***** DONE expose =workspace= in =Dataset= via GraphQL so it can be user-defined
***** DONE test data source put/get computer id
***** DONE test data source put/get latest snapshot
**** DONE implement ~delete data set~ usecase (and graphql mutation)
**** DONE migrate store implementations to be blob data sources
**** DONE try using cargo workspaces to speed up compilation
***** c.f. https://doc.rust-lang.org/book/ch14-03-cargo-workspaces.html
***** each unit compiles separately
***** tests move into the appropriate package
***** can run all tests or tests for one package (=cargo test -p <name>=)
***** database and pack store code go into separate library packages
***** bulk of code moves to binary package named =server=
**** DONE enable the main ~supervisor~ code to start at bootup
***** DONE migrate integ tests for backup manager
***** DONE migrate file restore code and tests
***** DONE migrate process management code and tests
***** DONE resolve the domain/data layer violation
****** add function to record repo to create pack repo for given dataset
****** pass the boxed record repo to the supervisor code, not the db_path
***** DONE write tests for =load_dataset_stores()= in repo impl
***** DONE start the supervisor in the main thread
***** DONE write unit tests for the process manager
***** DONE try converting process integ tests to unit tests
**** TODO implement ~get snapshot~ usecase (and graphql query)
**** TODO implement ~get tree~ usecase (and graphql query)
**** TODO convert tests in =schema_test.rs= to unit tests
**** TODO implement ~put back~ usecase (and graphql query)
***** this is the new ~put back~ feature, not the download file feature
**** TODO implement ~start backup~ usecase (and graphql mutation)
***** TODO design and implement the usecase to start a backup
**** TODO write a store validator to validate input to use cases
***** constructs the =Store= entity, passes to blob repo for validation
***** blob repo consults with the appropriate blob data source
***** each blob data source implementation then validates the store definition
** Update rusoto to latest
*** with 0.43 they use std future now instead of futures
*** maybe can remove the =futures= and =futures-fs= crates
**** actix may be using the futures crate, but likely can all be updated
*** look at the rusoto_s3 API documentation to figure out how to use it now
** Advanced Scheduling
*** TODO frontend
**** TODO Support multiple schedules in interface
**** TODO Support day-of-week in schedule
**** TODO Support day-of-month in schedule
**** TODO Support week-of-month in schedule
**** TODO Support time-range in schedule
*** TODO backend
**** Supervisor needs to consider start time in the time range (if any)
**** =BackupMaster= needs to calculate the stop time in =new()=
**** if stop time is reached in =BackupMaster.handle_file()= then raise =OutOfTimeError=
**** supervisor to report the =OutOfTimeError= a little better than normal errors
** Put Back feature
*** TODO write usecase for ~put back~ that restores file to original path
*** TODO add GraphQL mutation to invoke ~put back~ usecase
*** TODO Rename the "restore" button to "download"
*** TODO Add a "put back" button to invoke the GraphQL mutation
** Cloud Backup
*** TODO retry a pack upload if it encounters a temporary error
**** e.g. minio gets an md5 mismatch when storing an object
*** TODO support excluding certain file patterns from backup
**** part of dataset configuration
**** merge with the defaults in =engine.rs=
*** TODO add store that supports Google Cloud Storage
**** Check for bucket name collisions and retry in pack store
**** https://cloud.google.com/storage/docs/best-practices
*** TODO add form for defining a Google Cloud Storage store
*** TODO use this to replace =akashita= for online backups
**** define a new google cloud project named ~zorigami~
**** eventually remove the old ~akashita~ project and files
**** old akashita configuration file:
#+BEGIN_SRC erlang
{gcs_region, "us-west1"}.
{gcp_credentials, "/working/config/credentials.json"}.
{use_sudo, false}.
{go_times, ["07:00-13:00"]}. % times are UTC
{tmpdir, "/akashita"}.
{split_size, "128M"}.
{default_excludes, [".apdisk", ".DS_Store", ".localized", ".thumbnails"]}.

{buckets, [
    {"misc", [
        {dataset, "zeniba/shared"},
        {clone_base, "zeniba/akashita"},
        {paths, [
            "Antonia",
            "Applications",
            "Artwork",
            "Books",
            "Certificates",
            "Correspondence",
            "Documents",
            "Historical",
            "Medical",
            "Nathan",
            "Performances",
            "Projects",
            "Receipts",
            "Sounds",
            "Writings"
        ]},
        {compressed, true}
    ]},
    {"photos", [
        {dataset, "zeniba/shared"},
        {clone_base, "zeniba/akashita"},
        {paths, [
            "Pictures"
        ]}
    ]},
    {"videos", [
        {dataset, "zeniba/shared"},
        {clone_base, "zeniba/akashita"},
        {paths, [
            "Movies"
        ]}
    ]},
    {"tanuki", [
        {dataset, "zeniba/shared"},
        {clone_base, "zeniba/akashita"},
        {paths, [
            "tanuki"
        ]}
    ]}
]}.
#+END_SRC
** Loose backend issues
*** TODO the monthly fuzzy schedule test fails on the 30th of the month
*** TODO schema custom types need some unit tests
**** especially the schedule validation code
*** TODO sometimes =test_db_threads_one_path()= test fails getting a lock
**** seemingly only on Ubuntu (maybe Debian, need to test)
*** TODO file restore in =main.rs= should schedule cleanup of the temporary file
**** supervisor could delete anything last modified an hour ago
*** TODO need to schedule pruning old database snapshots on remote store
**** there is no deduplication of the database files, so it uses more space
**** only really need the most recent copy
**** be mindful of remote storage deletion policies
** Loose GraphQL tasks
*** TODO test the GraphQL schema and resolvers
**** TODO "integers" that are not radix 10 integers
**** TODO digests that lack the proper algorithm prefix
**** TODO querying for things when there is nothing in the database
**** TODO querying snapshots
**** TODO querying trees
**** TODO querying files
**** DONE fetching configuration record
**** TODO updating configuration record
**** DONE querying datasets
**** DONE mutating datasets
**** DONE querying stores
**** DONE mutating stores
*** TODO probably should use a better client cache
**** c.f. =graphql_flutter= example that implements a =uuidFromObject()= function
**** uses the "type" of the object and its unique identifier as the caching key
**** our objects would need to have a "typename" for this to work
*** TODO find out how to document arguments to mutations
**** c.f. juniper API docs: Attribute Macro juniper::object
*** TODO handle errors in getting Database ref in graphql handler
** Loose WebUI tasks
*** TODO improve the navigation drawer
**** currently selected option should be highlighted, not actionable
*** TODO improve (server) error handling
**** when a temporary server error occurs, offer a "Retry" button
*** TODO show a message when there are no datasets, packs, snapshots
*** TODO improve snapshot tree browser
**** for larger number of entries, should use =PaginatedDataTable=
**** nice to have: sticky table header
**** nice to have: sort by file type
*** TODO consider how to hide the minio secret key using a show/hide button
*** TODO consider l10n
**** c.f. https://resocoder.com/2019/06/01/flutter-localization-the-easy-way-internationalization-with-json/
*** TODO dataset status says "running" even though it had an error
**** need to expose the error status via GraphQL
*** TODO improve the data sets form
**** TODO use the =validate()= function on =DataSet= to ensure validity
**** TODO should decode the computer ID to improve readability
**** TODO would be better to have frequency options in a row
***** need to use =FormBuilderCustomField= and return a =Row= of widgets
*** TODO should sort the datasets so they are always in the same order
**** maybe sort them by date, with most recent first
*** TODO tree entries of =ERROR= type should be displayed as such
**** error message from =TreeEntry.new()= could be stored as a new type of =TreeReference=
***** e.g. =TreeReference.ERROR(String)= where the string is the error message
*** TODO should have ui for listing all snapshots in a dataset
**** probably need paging in the ui and graphql api
**** consider presenting in a style similar to Time Machine
*** TODO improve the page for defining datasets
**** TODO store selection should be easier for the user
**** TODO disable Save button until form is valid
**** TODO store input validator should check stores actually exist
**** TODO pack size should have minimum and maximum values
*** TODO improve the page for defining stores
**** TODO disable Save button until form is valid
**** TODO delete button should be far away from the other button(s)
**** TODO delete button should require two clicks, with "are you sure?"
**** TODO display help text on stores page when there are no stores defined
**** TODO display help text on home page when there are no datasets defined
**** TODO scroll to form when edit button is clicked
***** with a bunch of stores on the screen, click ~Edit~ for last one
***** page refreshes and scrolls to the top
**** TODO autofocus input field on edit
***** this is tricky with React, =autofocus= is not really honored
***** can do it if we turn the input element into a full-fledged component
***** and use the =useRef()= hook to set the focus on the HTML element
***** c.f. https://reactjs.org/docs/hooks-reference.html#useref
*** TODO use breadcrumbs in the tree navigator to get back to parent directories
*** TODO consider and improve accessibility
**** enable testing for a11y sanity
**** add hints to improve the presentation of information
***** configuration panel
***** snapshot browser
*** TODO font assets seem to go in the wrong directory
**** ends up in =build/web/assets/assets/fonts= instead of =build/web/assets/fonts=
**** but it works anyway because =FontManifest.json= has the correct paths
** Multi-file Restore
*** backend
**** receive requests of tree/file digests to be restored
**** only works for "put back" feature, not the "download" to browser
**** limit the number of digests that can be submitted in one request
**** return a request ID to the client for querying later
**** add the requests to a limited size queue to prevent abuse
**** process the requests in series on another thread (via actix)
**** need to ensure packs are downloaded only once
***** collect all of the file/chunk digests, then collect all of the pack digests
***** cycle through the packs, downloading, extracting, discarding extra chunks
***** assemble the files at the end (or as the chunks become available?)
**** use application state to track progress of the requests
**** client requests status of restore using request ID
**** requests can be canceled
**** restore procedure needs to check request status in case canceled
*** frontend
**** write a usecase for submitting the tree/file digests
**** write a usecase for checking the progress of the request
**** direct user to a "restore requests" page that shows progress
** Manual Backup controls
*** add a "backup now" button to datasets listing
**** need a GraphQL mutation to signal backup to start
**** add a =start_dataset_now()= in =supervisor= module, similar to =start_due_datasets()=
***** that is, enqueue =StartBackup= on the =Runner= actor
*** similiarly have a "stop backup" button if it is running
**** need a GraphQL mutation to signal backup to stop
**** add a =StopBackup= action in =state= module
**** the =StopBackup= action sets =stop_requested= in =BackupState=
**** then =handle_file()= in =engine= module calls =get_state()= and checks for =stop_requested=
**** =handle_file()= will return an error if =stop_requested= is true
*** consider how one might "pause" a backup in progress
** Initial Configuration
*** Allow user to set user/host names for computer UUID
**** They may need to avoid naming conflicts with other local users
**** Imagine a computer lab all sharing a single cloud storage account
** Snapshot Pruning
*** Should probably not run collection while a backup is in progress
*** Consider how to prune old snapshots, pack files, etc
**** Time Machine and Attic both have retention policies
**** Based on retention policy, delete stale snapshot records
***** set the child's parent reference to skip over stale snapshot
**** Use some form of "mark and sweep" to find dangling records
***** discover all of the unreachable records (see below)
***** remove unreachable snapshot, tree, file, and chunk records
***** mark all unreachable pack records as ready for removal
**** Find all marked pack records older than N days, delete pack files
***** that is, many cloud providers charge extra for deleting archives too soon
***** pack store can suggest a number of days since that is often already known
****** i.e. Google and Amazon have infrequently changing, published policies
***** user can specify their own value for each pack store if necessary
*** Strategies for finding unreachable objects
**** Mark and Sweep using a separate database instance
***** use another database instance for tracking reachable objects
***** scan production database for reachable objects
****** datasets -> snapshots -> trees -> files -> chunks -> packs
***** mark every reachable object by putting its key in the other database
***** prune everything from the production database that was not marked
***** once the garbage collection is done, delete the temporary database
**** Build lists of reachable objects in memory
***** could blow up if there are many database records
***** avoids writing to the production database
**** Mark and Sweep but with database records
***** avoids blowing up memory for very large databases
***** results in many database writes and level compaction
*** Implementation should follow Clean Architecture to improve testability
**** entities and use case separated from data sources via repositories
**** this allows for easily mocking up data to feed the pruning use case
***** i.e. when the use case asks for trees and such, give it mock data structures
** More Functionality
*** TODO Perform a full backup on demand, discard all previous backups
**** Wifey doesn't like the idea of accumulating old stuff
**** Gives the user a chance to save space by removing old content
*** TODO event dispatching for the web and desktop
**** use the state management to manage "events" and state
**** engine emits actions/events to the store
***** for backup and restore functions
***** e.g. "downloaded a pack", "uploaded a pack"
**** store holds the cumulative data so late attachers can gather everything
**** supervisor threads register as subscribers to the store
**** clients will use GraphQL subscriptions to receive updates
**** supervisor threads emit GraphQL subscription events
*** TODO consider how datasets can be modified after creation
**** cannot change stores assigned to dataset once there are snapshots
**** basically would require starting over if changing stores, base path, etc
*** TODO consider how to restore symbolic links
**** i.e. no file chooser to download anything
**** what if the same path is now a file/directory?
*** TODO Secure FTP improvements
**** TODO support SFTP with private key authentication
***** use store form to take paths for public and private keys
**** TODO allow private key that is locked with a passphrase
***** passphrase for private key would be provided by envar
** More Information
*** TODO Show details about snapshots and files
**** show differences between two snapshots
**** show pack/chunk metrics for   all   files in a snapshot
**** show pack/chunk metrics for changed files in a snapshot
*** TODO Query to see histogram of file sizes, number of chunks, etc
**** for a given snapshot
***** count number of files with N chunks for all values of N
*** TODO Show number of packs stored in a pack store
**** would have to keep track in the database
** Architecture Review
*** Database migrations
**** Use the =serde= crate features (c.f. https://serde.rs)
**** Use =#[serde(default)]= on struct to fill in blanks for new fields
**** Add =#[serde(skip_serializing)]= to a deprecated struct field
**** New fields will need accessors that convert from old fields as needed
***** reset the old field to indicate it is no longer relevant
**** Removing a field is no problem for serde
*** Embedded Database
**** Is the default RocksDB performance sufficient?
**** Consider https://github.com/spacejam/sled/
***** written in Rust, open source
***** will need prefix key scanning
****** looks like you just use a prefix of the key (sorts before the matching keys)
*** Client/Server
**** Look at ways to secure the server, to allay fears of exploits
**** A web conferencing tool was exploited via its hidden HTTP server
** Desktop application
*** design a configuration system for desktop
**** define the whole clean architecture setup
***** entities, use cases, repositories
**** data source for web will have values defined by environment_config only
**** data source for desktop will use shared preferences (?) for persistence
**** data layer repository chooses between data sources based on environment
***** how to detect if application was compiled for web
#+BEGIN_SRC dart
import 'package:flutter/foundation.dart' show kIsWeb;
if (kIsWeb) { /* web stuff */ } else { /* not web */ }
#+END_SRC
*** clipboard support
**** look for clipboard plugin for flutter (for macOS)
**** c.f. https://flutter.dev/docs/development/packages-and-plugins/developing-packages
** macOS support
*** TODO Use =launchd= to manage the process, have it start automatically
*** TODO optional Time Machine style backup and retention policy
**** hourly backups for 24 hours
**** daily backups for 30 days
**** weekly backups for everything else
**** prune backups to maintain a certain size
*** TODO Use this to replace Time Machine (store on server using minio)
** Full Restore
*** Procedure for full restore
**** User installs and configures application
**** User invokes "full restore" function
**** User provides a temporary pack store configuration
**** Query pack store to get candidate computer UUID values
**** User chooses database to restore
***** if current UUID matches one in the available set, select it by default
**** Fetch the most recent database files
***** Restore to a different directory, then copy over records
***** Copy every record except for =configuration= (and maybe others?)
***** Copy records for datasets, stores, snapshots, packs, etc
**** User can now browse datasets and restore as usual
**** Restoring an entire dataset is simply the "tree restore" case
*** Walk the user through the process
**** Configure the primary pack store for retrieval
**** Inform user that this pack store configuration is only temporary
**** Select database to retrieve based on computer UUID
**** Instruct user to restore as usual from dataset(s)
*** TODO Restore file attributes from tree entry
**** TODO File mode
**** TODO File user/group
**** TODO File extended attributes
*** TODO Restore directories from snapshot
**** TODO Directory mode
**** TODO Directory user/group
**** TODO Directory extended attributes
**** TODO Restore multiple files efficiently
**** TODO Restore a directory tree efficiently
*** TODO Detect and prune stale snapshots that never completely uploaded
**** Stale snapshots exist in the database but are not referenced elsewhere
*** TODO Support snapshots consisting only of mode/owner changes
**** i.e. no file content changes, just the database records
** Windows support
*** TODO Support Windows file types
**** ReadOnly
**** Hidden
**** System
** More Better
*** TODO document how the user might change the passphrase over time
**** user must remember their old passwords in order to decrypt old pack files
**** the application will never store the actual password anywhere
**** will need to prompt the user when a different passphrase is needed
*** TODO support database integrity checks
**** ensure all referenced records actually exist
**** like git fsck, start at the top and traverse everything
**** find and report dangling objects
**** an automated scan could be run on occasion
*** TODO Automatically prune backups more then N days old
**** For Google and Amazon, anything older than 90 days is free to remove
*** TODO Option to keep N daily, M weekly, and P monthly backups (a la Attic backup)
*** TODO Permit scheduling upload hours for each day of the week
**** e.g. from 11pm to 6am Mon-Fri, none on Sat/Sun
*** TODO Command-line option to dump database to json (separate by key prefix, e.g. ~chunk~)
*** TODO Support deduplication across multiple computers
**** Place the chunks and packs in a seperate "database" for syncing
***** For RocksDB, use a column family if it helps with =GetUpdatesSince()=
**** RocksDB replication story as of 2019-02-20:
: Q: Does RocksDB support replication?
: A: No, RocksDB does not directly support replication. However, it offers
: some APIs that can be used as building blocks to support replication.
: For instance, GetUpdatesSince() allows developers to iterate though all
: updates since a specific point in time.
***** see =GetUpdatesSince()= and =PutLogData()= functions
**** User configures the host name of the ~peer~ installation
***** Use that to form the URL with which to =sync=
**** Share the chunks and packs documents with a ~peer~ installation
**** At the start of backup, sync with the ~peer~ to get latest chunks/packs
*** TODO Consider how to deal with partial uploads
**** e.g. Minio/S3 has a means of handling these
*** TODO Pack store should recommend pack sizes
**** e.g. Glacier recommends archives greater than 100mb
**** can only really make a recommendation, the user has to choose the right size
*** TODO Permit removing a store from a dataset
**** would encourage user to clean up the remote files
**** for local store, could remove the files immediately
**** must invalidate all of the snapshots effected by the missing store
*** TODO Permit moving from one store to another
**** would mean downloading the packs and uploading them to the new store
*** TODO Support Amazon S3
**** Minio seems to have no bucket limit (higher than 100)
**** Need to limit number of remote buckets to 100
**** Bucket limit: catch the error and handle by re-using another bucket
*** TODO Support Amazon Glacier
**** Need to limit number of remote buckets to 1000
**** Use S3 to store the database-to-archive mapping of each snapshot
**** Offer user option to use "expedited" retrievals so they go faster
*** TODO Support Amazon Cloud Drive
*** TODO Support Microsoft Azure blob storage
*** TODO Support Backblaze B2
*** TODO Support [[https://wiki.openstack.org/wiki/Swift][OpenStack Swift]]
*** TODO Support Wasabi
*** TODO Support Google Drive
*** TODO Support Google Cloud Coldline
*** TODO Support Dropbox
*** TODO Support Oracle Cloud Storage
*** TODO Support IBM Cloud Storage
*** TODO Support Rackspace Cloud Files
*** TODO Consider how to backup and restore FIFO, BLK, and CHR "files"
**** c.f. https://github.com/jborg/attic/blob/master/attic/archive.py
**** c.f. https://github.com/avz/node-mkfifo (for FIFO)
**** c.f. https://github.com/mafintosh/mknod (for BLK and CHR)
* Product
** TODO Evaluate other backup software
*** TODO Check out some on App Store
**** Backup Guru LE
**** ChronoSync Express
**** Backup
**** Remote Backup Magic
**** Sync - Backup and Restore
**** Backup for Dropbox
**** Freeze - for Amazon Glacier
*** Lot of "folder sync" apps out there
** TODO Define the target audience
*** Average home user, no technical expertise required
** TODO Need distinquishing features
*** What sets this application apart from the other polished products?
**** Cross-platform (e.g. macOS, Windows)
**** Linux server ready
** Windows Certified
*** CloudBerry(?) has bunches of certifications
*** is that really so meaningful? *I* never cared
** Name
*** Joseph suggests "Attic"
**** =atticapp.com= is taken
**** =attic.app= is for sale
**** Look for ~attic~ in different languages
**** Esperanto: ~mansardo~
***** also means something in Macedonian
**** Hawaiian: ~kaukau~
**** Latin: ~atticae~
* Technical Information
** Data Growth
*** Database backup tgz seems to grow 8mb in 6 months
** JS Build Artifacts
*** Flutter => main.dart.js
| State      |    Size |
|------------+---------|
| production | 1742125 |
*** ReasonML + Webpack => main.js
| State       |    Size |
|-------------+---------|
| development | 2761882 |
| production  |  536345 |
| gzipped     |  145785 |
** Exploring other languages
*** Compile to native for easy deployment
*** Compile to native for code obfuscation
*** Rust
**** Advantages
***** compile to native
***** expressive, safe type system
***** good dependency management
***** lots of useful tools (e.g. clippy)
**** Disadvantages
***** fewer libraries compared to Go
**** DONE GraphQL server
***** Make sure it can generate a schema.json
***** Should be able to parse schema definition (for docs)
***** https://github.com/graphql-rust/juniper (BSD)
****** supports entire GraphQL specification
****** does /not/ read GraphQL schema language
****** supports GraphiQL and Playground
****** is not the HTTP server, but integrates with them
****** uses macros for schema documentation
***** tutorial at [[http://alex.amiran.it/post/2018-08-16-rust-graphql-webserver-with-warp-juniper-and-mongodb.html][alex.amiran.it]] that uses warp web framework
***** old https://github.com/nrc/graphql (MIT/Apache)
**** DONE Web framework
***** our needs are simple, so a simple framework is best
***** Actix https://actix.rs (Apache 2.0)
****** works with stable Rust
****** powerful and easy to use
****** testing library
****** integrates with juniper
****** offers state management for web code
****** lot more actively used than warp
***** warp https://github.com/seanmonstar/warp (MIT)
****** works with stable Rust
****** powerful and easy to use
****** testing library
****** integrates with juniper
***** Rocket https://rocket.rs (Apache 2.0)
****** requires Rust nightly because of fancy macros
****** routing using macros
****** streams input and output
****** cookies
****** json
****** environment configuration
****** testing library
****** integrates with juniper
***** Gotham https://gotham.rs (MIT/Apache 2.0)
****** targets stable Rust
****** routing
****** middleware
****** sessions
****** cookies
****** templates
****** testing library
****** how to integrate with juniper is unknown
***** Iron http://ironframework.io (MIT)
****** crate has not been updated since 2017
****** everything is middleware that must be added in
****** integrates with juniper
***** Nickel http://nickel-org.github.io (Express.js like) (MIT)
****** pretty basic compared to Rocket
***** tower-web https://github.com/carllerche/tower-web (MIT)
****** competing with warp? hyper?
**** DONE Database
***** ideally want something well maintained, reliable
***** schema is pretty simple, could use key/value store
***** RocksDB https://github.com/rust-rocksdb/rust-rocksdb (Apache)
****** statically links everything, including compression support
***** SQLite https://github.com/jgallagher/rusqlite (MIT)
***** Rust wrapper to LevelDB https://github.com/skade/leveldb
***** LevelDB in Rust (active?) https://bitbucket.org/dermesser/leveldb-rs/overview
**** DONE dotenv
***** https://github.com/dotenv-rs/dotenv (MIT)
**** DONE Configuration
***** https://github.com/mehcode/config-rs (MIT/Apache)
**** DONE =getpwuid= and =getgrgid= support
***** libc: https://crates.io/crates/libc (MIT/Apache 2.0)
**** DONE test library
***** https://github.com/rust-rspec/rspec (MPL-2.0)
****** appears to be dead
***** https://github.com/utkarshkukreti/speculate.rs (MIT)
****** works well for integration tests
**** DONE UUID support
***** https://github.com/uuid-rs/uuid (MIT/Apache 2.0)
**** DONE xattr support
***** Unix only: https://github.com/Stebalien/xattr (MIT/Apache 2.0)
**** DONE CDC
***** https://github.com/jrobhoward/quickcdc (MIT/Apache 2.0)
****** not quite FastCDC, given dates of paper, but should be close enough
****** use a constant salt value for predictable results
****** example uses =memmap= crate to read large files
**** DONE Tar file
***** https://github.com/alexcrichton/tar-rs (MIT/Apache 2.0)
**** DONE PGP/Encryption
***** https://github.com/gpg-rs/gpgme (LGPL)
****** will need to bundle the =gpgme= library (unless statically linked)
***** cryptostream https://github.com/neosmart/cryptostream (MIT)
***** basic packets [[https://github.com/csssuf/pretty-good][csssuf/pretty-good]]
***** read only [[https://nest.pijul.com/pmeunier/openpgp][pijul]] openpgp
**** DONE ULID
***** https://crates.io/crates/rusty_ulid (MIT)
**** DONE SFTP client
***** https://github.com/alexcrichton/ssh2-rs (MIT/Apache 2.0)
**** DONE AWS client
***** Rusoto https://www.rusoto.org (MIT)
**** DONE Google Cloud client
***** https://github.com/Byron/google-apis-rs (MIT/Apache 2.0)
**** DONE Minio client
***** Rusoto supports Minio https://github.com/rusoto/rusoto (MIT)
*** Go vs Rust
**** Go: first class support for cloud services
**** Go: statically linked OpenPGP readily available
**** Go: easy to read and write language
**** Rust: mature dependency management tooling
**** Rust: cargo has good editor support
**** Rust: expressive type system
**** Rust: nominal subtyping is much easier to follow
**** Rust: streamlined error handling
**** Rust: fine-grained namespaces and visibility control
