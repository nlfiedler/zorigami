* Tasks
** Better file/tree Restore
*** current limitations
**** put-back waits for the result with a modal spinner
**** put-back only operates on a single file at a time
*** desired functionality
**** restore a single file or tree of files per request
**** show the status of multiple restoration requests
**** cancel some or all restoration requests
*** use cases
**** add file/tree digests to the ~incoming~ queue
**** query status of files in the ~pending~ queue
**** cancel some/all requests in ~pending~ queue
*** backend
**** memory constraint: do not queue up _all_ files at one time
***** process the tree one level at a time
**** receive request with digests of trees and/or files to be restored
**** add the request to a queue of requests to be processed
**** supervised actor wakes up on an interval(?) to process requests
***** pop a request from the request queue
***** if file request, add to file queue
***** if tree request, fetch record, add entries to file/tree queues
***** process entries on file queue: fetch packs, assemble, etc
***** process entries on tree queue: files to file queue, trees to tree queue
***** when there are no more requests queued to process, clean up chunk files
**** status query handler
***** any request that cannot be found is assumed to have completed
**** database restore changes
***** check if restore has any pending requests, error out if non-empty
**** eventually could be sensitive to disk usage, prune packs/chunks
***** may need to download the packs again, but saves space in the mean time
*** actor-based approach (that is not workable)
**** receive request with digest of tree or file to be restored
***** send ~incoming~ message to incoming actor with request object
***** return a request ID to the client for querying later
**** incoming actor: process incoming requests
***** receive ~incoming~ message with request object
***** if request for a file, fetch record to get chunks
***** if request for a tree, enqueue file entries and subtrees as new requests
***** send ~pending~ message to assemble actor with file records
***** look up the chunk records to get the packs
***** send ~download~ message to download actor with pack records
**** recurse actor: convert tree entries into ~incoming~ messages
***** for each file entry, create file-based request and send to incoming actor
***** for each tree entry, create tree-based request and send to incoming actor
**** download actor: process packs on ~download~ queue
***** receive ~download~ message and consolidate into a hash set
***** use an interval timer to wake up and check the queue periodically
***** dequeue pack record, download pack, extract chunks
***** send ~chunks available~ message to pending actor
**** assemble actor: consolidate chunks into files
***** receive ~pending~ message with file records
****** add file records to ~pending~ queue
***** receive ~chunks available~ message
****** scan available chunks to try to assemble files on ~pending~ queue
****** when a file is completed, move record to ~completed~ list
**** implement this restore functionality as a kind of ~manager~
***** define an interface for mocking in unit tests
***** consider how this design might factor into managing backups
**** when to completed requests age out of the system?
*** frontend
**** write a usecase for submitting the tree/file digests
**** write a usecase for checking the progress of the request
**** direct user to a "restore requests" page that shows progress
**** write a new bloc for this restore management
**** consider how to show errors with respect to files
**** maybe show list of all files being restored, with status/error in a table
** Manual Backup controls
*** write use cases for starting and stopping a backup
*** add a "backup now" button to datasets listing
**** need a GraphQL mutation to signal backup to start
**** add a =start_dataset_now()= in =supervisor= module, similar to =start_due_datasets()=
***** that is, enqueue =StartBackup= on the =Runner= actor
*** similiarly have a "stop backup" button if it is running
**** need a GraphQL mutation to signal backup to stop
**** add a =StopBackup= action in =state= module
**** the =StopBackup= action sets =stop_requested= in =BackupState=
**** then =handle_file()= in =engine= module calls =get_state()= and checks for =stop_requested=
**** =handle_file()= will return an error if =stop_requested= is true
*** consider how one might "pause" a backup in progress
** Improved interface
*** c.f. https://duplicacy.com/guide.html webui looks good
** Loose backend issues
*** TODO testing the minio? pack store showed a tokio runtime error
*** TODO refine use of =&str= and =String= arguments by using =Into<String>=
**** note that using =Cow= helps to minimize copying
#+BEGIN_SRC rust
pub fn name<T: Into<String>>(mut self, name: T) -> Self {
    self.name = Cow::Owned(name.into());
    self
}
#+END_SRC
*** TODO Too many open files (in RocksDB)
**** need to set =set_max_open_files()= on database options
**** default ulimit on macOS is 256, so something less would be ideal
**** ran out of files in tanuki when rocksdb directory contained 217 files
**** maybe consider a means of countering this error at runtime
*** TODO the monthly fuzzy schedule test fails on the 30th of the month
** Loose GraphQL tasks
*** TODO update juniper to latest release; 0.15 API changes substantially
*** TODO schema custom types need some unit tests
**** especially the schedule validation code
*** TODO probably should use a better client cache
**** c.f. =graphql_flutter= example that implements a =uuidFromObject()= function
**** uses the "type" of the object and its unique identifier as the caching key
**** our objects would need to have a "typename" for this to work
*** TODO find out how to document arguments to mutations
**** c.f. juniper API docs: Attribute Macro juniper::object
** Loose WebUI tasks
*** TODO pack store ~test~ feature shows snackbar repeatedly
*** TODO sometimes get an HTTP error in GraphQL client
**** should automatically retry the query a few times before giving up
*** TODO test with a smaller browser window to surface sizing issues
*** TODO when there are no snapshots, clicking the dataset row does nothing
*** TODO how to refresh the snapshots screen?
**** gets stale as soon as a backup has been run
**** navigation to the snapshots does not work if there were none to start with
**** maybe add a refresh button like in google cloud console
*** TODO schedule start/stop times should be using local time (no excuse for not doing this)
*** TODO local store basepath and google credentials should use file picker
**** https://pub.dev/packages/form_builder_file_picker
*** TODO improve the navigation drawer
**** currently selected option should be highlighted, not actionable
*** TODO improve (server) error handling
**** when a temporary server error occurs, offer a "Retry" button
*** TODO improve snapshot tree browser
**** should sort entries by filename case-insensitively
**** for larger number of entries, should use =PaginatedDataTable=
**** nice to have: sticky table header
**** nice to have: sort by file type
*** TODO consider how to hide the minio secret key using a show/hide button
*** TODO consider approaches to l10n and i18n
**** c.f. https://resocoder.com/2019/06/01/flutter-localization-the-easy-way-internationalization-with-json/
*** TODO improve the data sets form
**** TODO FAB covers the =DELETE= button even when scrolled all the way down
**** TODO use the =validate()= function on =DataSet= to ensure validity
**** TODO should decode the computer ID to improve readability
*** TODO should sort the datasets so they are always in the same order
**** maybe sort them by date, with most recent first
*** TODO tree entries of =ERROR= type should be displayed as such
**** error message from =TreeEntry.new()= could be stored as a new type of =TreeReference=
***** e.g. =TreeReference.ERROR(String)= where the string is the error message
*** TODO should have ui for listing all snapshots in a dataset
**** consider presenting in a style similar to Time Machine
**** e.g. a timeline of the snapshots
**** c.f. https://pub.dev/packages/flutter_timeline
**** probably need paging in the ui and graphql api
*** TODO improve the page for defining stores
**** TODO delete button should be far away from the other button(s)
**** TODO delete button should require two clicks, with "are you sure?"
*** TODO use breadcrumbs in the tree navigator to get back to parent directories
*** TODO consider and improve accessibility
**** enable testing for a11y sanity
**** add hints to improve the presentation of information
***** configuration panel
***** snapshot browser
** Flutter and form builder
*** See comments in =pubspec.yaml= regarding =flutter_form_builder=
*** Wait for flutter form builder to update to Flutter 2 stable
** Initial Configuration
*** Walk user through pack store and data set creation
*** Offer path for restoring database from existing pack store
*** Allow user to set user/host names for computer UUID
**** They may need to avoid naming conflicts with other local users
**** Imagine a computer lab all sharing a single cloud storage account
** Snapshot Pruning
*** Use a multi-phased approach with pruning and garbage collection
*** Must not run collection while a backup is in progress
*** Must prevent a backup from starting while pruning is in progress
*** Phase 1: prune snapshots based on a policy
**** set the child's parent reference to skip over stale snapshot
**** e.g. remove snapshots more then N days old
**** e.g. keep N snapshots per day, M per week, and P per month
*** Phase 2: prune unreachable objects in the database
**** copy everything reachable to a new database instance
****** datasets -> snapshots -> trees -> files -> chunks -> packs
**** delete the old database
*** Phase 3: prune unreferenced packs from pack store
**** honor cloud data rentetion policies
***** e.g. typically anything older than 90 days costs nothing to delete
***** Google has different minimum storage durations for each storage class
****** https://cloud.google.com/storage/docs/storage-classes
***** user can specify their own value for each pack store if necessary
*** Phase 4: prune old database snapshots (no need to keep old copies)
**** honor cloud data rentetion policies
**** use the =upload_time= in the =Pack= record to determine age
*** Implementation should follow Clean Architecture to improve testability
**** entities and use case separated from data sources via repositories
**** this allows for easily mocking up data to feed the pruning use case
***** i.e. when the use case asks for trees and such, give it mock data structures
** Error handling
*** Consider a structured design for error types and handling
*** Look at https://github.com/dtolnay/thiserror for defining error types
*** May improve error handling and reporting
** Advanced Scheduling
*** backend
**** Permit ~hourly~ backups every N hours
**** Permit ~daily~ backups every N days
**** Permit ~weekly~ backups every N weeks
**** Permit ~monthly~ backups every N months
*** frontend
**** TODO Support multiple schedules in interface
**** TODO Support day-of-week in schedule
**** TODO Support day-of-month in schedule
**** TODO Support week-of-month in schedule
**** TODO Support time-range in schedule
** More Functionality
*** TODO search snapshots to find a file/directory by a given pattern
**** the file/dir is not in the latest snapshot but some older one, go find it
**** might not even know the full path of the file/dir in question
*** TODO support excluding certain file patterns from backup
**** part of dataset configuration
**** merge with the defaults in =backup.rs=
*** TODO Perform a full backup on demand, discard all previous backups
**** Wifey doesn't like the idea of accumulating old stuff
**** Gives the user a chance to save space by removing old content
**** Optionally prune all existing packs in the process
*** TODO event dispatching for the web and desktop
**** use the state management to manage "events" and state
**** engine emits actions/events to the store
***** for backup and restore functions
***** e.g. "downloaded a pack", "uploaded a pack"
**** store holds the cumulative data so late attachers can gather everything
**** supervisor threads register as subscribers to the store
**** clients will use GraphQL subscriptions to receive updates
**** supervisor threads emit GraphQL subscription events
*** TODO consider how datasets can be modified after creation
**** cannot change stores assigned to dataset once there are snapshots
**** basically would require starting over if changing stores, base path, etc
*** TODO consider how to restore symbolic links
**** i.e. no file chooser to download anything
**** what if the same path is now a file/directory?
*** TODO Secure FTP improvements
**** TODO support SFTP with private key authentication
***** use store form to take paths for public and private keys
**** TODO allow private key that is locked with a passphrase
***** passphrase for private key would be provided by envar
*** TODO Repair missing pack files in pack stores
**** expose the GraphQL operation via the graphical interface
** More Information
*** TODO Show details about snapshots and files
**** show differences between two snapshots
**** show pack/chunk metrics for   all   files in a snapshot
**** show pack/chunk metrics for changed files in a snapshot
*** TODO Query to see histogram of file sizes, number of chunks, etc
**** for a given snapshot
***** count number of files with N chunks for all values of N
*** TODO Show number of packs stored in a pack store
** Database Integrity
*** support database integrity checks
**** ensure all referenced records actually exist
**** like git fsck, start at the top and traverse everything
**** find and report dangling objects
**** an automated scan could be run on occasion
** Pack file integrity
*** Retrieve random pack files and verify integrity
** Architecture Review
*** Database per dataset directory
**** Centralized configuration in a known location
***** would default to something sensible in user home directory
***** overridden by environment variable
***** JSON or XML formatted plain text file
***** Holds paths to the various data sets
***** Holds pack store configuration
**** Each data set directory has a database directory (and backup)
**** Backup process automatically excludes the database and its backup
**** What would a full restore procedure look like?
**** Benefits
***** reduced risk in the event of database corruption
**** Drawbacks
***** additional disk usage for database overhead
***** forces user to keep database with the dataset
*** Parallel backups
**** Currently the backup supervisor spawns a single thread (=Arbiter=) to manage backups
**** This causes all backups to be serialized
**** For parallel backups, would use the =SyncArbiter= from actix
*** Database migrations
**** Use the =serde= crate features (c.f. https://serde.rs)
**** Use =#[serde(default)]= on struct to fill in blanks for new fields
**** Add =#[serde(skip_serializing)]= to a deprecated struct field
**** New fields will need accessors that convert from old fields as needed
***** reset the old field to indicate it is no longer relevant
**** Removing a field is no problem for serde
*** Shared pack repository
**** Current design basically forces each user/install to have a separate pack repo
**** Otherwise the pack pruning would delete the packs for other users saving to the same repo
*** Embedded Database
**** Is the default RocksDB performance sufficient?
**** Consider https://github.com/spacejam/sled/
***** written in Rust, open source
***** will need prefix key scanning
****** looks like you just use a prefix of the key (sorts before the matching keys)
***** will need backup/restore functions
*** Client/Server
**** Look at ways to secure the server, to allay fears of exploits
**** A web conferencing tool was exploited via its hidden HTTP server
** Desktop application
*** design a configuration system for desktop
**** define the whole clean architecture setup
***** entities, use cases, repositories
**** data source for web will have values defined by environment_config only
**** data source for desktop will use shared preferences (?) for persistence
**** data layer repository chooses between data sources based on environment
***** how to detect if application was compiled for web
#+BEGIN_SRC dart
import 'package:flutter/foundation.dart' show kIsWeb;
if (kIsWeb) { /* web stuff */ } else { /* not web */ }
#+END_SRC
*** clipboard support
**** look for clipboard plugin for flutter (for macOS)
**** c.f. https://flutter.dev/docs/development/packages-and-plugins/developing-packages
** macOS support
*** TODO Use =launchd= to manage the process, have it start automatically
*** TODO optional Time Machine style backup and retention policy
**** hourly backups for 24 hours
**** daily backups for 30 days
**** weekly backups for everything else
**** prune backups to maintain a certain size
*** TODO Use this to replace Time Machine (store on server using minio)
** Full Restore
*** Procedure for full restore
**** User installs and configures application
**** User invokes "full restore" function
**** User provides a temporary pack store configuration
**** Query pack store to get candidate computer UUID values
**** User chooses database to restore
***** if current UUID matches one in the available set, select it by default
**** Fetch the most recent database files
***** Restore to a different directory, then copy over records
***** Copy every record except for =configuration= (and maybe others?)
***** Copy records for datasets, stores, snapshots, packs, etc
**** User can now browse datasets and restore as usual
**** Restoring an entire dataset is simply the "tree restore" case
*** Walk the user through the process
**** Configure the primary pack store for retrieval
**** Inform user that this pack store configuration is only temporary
**** Select database to retrieve based on computer UUID
**** Instruct user to restore as usual from dataset(s)
*** TODO Restore file attributes from tree entry
**** TODO File mode
**** TODO File user/group
**** TODO File extended attributes
*** TODO Restore directories from snapshot
**** TODO Directory mode
**** TODO Directory user/group
**** TODO Directory extended attributes
**** TODO Restore multiple files efficiently
**** TODO Restore a directory tree efficiently
*** TODO Detect and prune stale snapshots that never completely uploaded
**** Stale snapshots exist in the database but are not referenced elsewhere
*** TODO Support snapshots consisting only of mode/owner changes
**** i.e. no file content changes, just the database records
** Windows support
*** TODO Backup files opened by a running process
**** Normally cannot read files that are opened on Windows
**** See Volume Shadow Copy Services (VSS) for details
*** TODO Support Windows file types
**** ReadOnly
**** Hidden
**** System
** More Better
*** TODO Permit scheduling upload hours for each day of the week
**** e.g. from 11pm to 6am Mon-Fri, none on Sat/Sun
*** TODO Command-line option to dump database to json (separate by key prefix, e.g. ~chunk~)
*** TODO Support deduplication across multiple computers
**** Place the chunks and packs in a seperate "database" for syncing
***** For RocksDB, use a column family if it helps with =GetUpdatesSince()=
**** RocksDB replication story as of 2019-02-20:
: Q: Does RocksDB support replication?
: A: No, RocksDB does not directly support replication. However, it offers
: some APIs that can be used as building blocks to support replication.
: For instance, GetUpdatesSince() allows developers to iterate though all
: updates since a specific point in time.
***** see =GetUpdatesSince()= and =PutLogData()= functions
**** User configures the host name of the ~peer~ installation
***** Use that to form the URL with which to =sync=
**** Share the chunks and packs documents with a ~peer~ installation
**** At the start of backup, sync with the ~peer~ to get latest chunks/packs
*** TODO Consider how to deal with partial uploads
**** e.g. Minio/S3 has a means of handling these
*** TODO Permit removing a store from a dataset
**** would encourage user to clean up the remote files
**** for local store, could remove the files immediately
**** must invalidate all of the snapshots effected by the missing store
*** TODO Permit moving from one store to another
**** would mean downloading the packs and uploading them to the new store
*** TODO Support Amazon S3
**** Minio seems to have no bucket limit (higher than 100)
**** Need to limit number of remote buckets to 100
**** Bucket limit: catch the error and handle by re-using another bucket
*** TODO Support Amazon Glacier
**** Need to limit number of remote buckets to 1000
**** Use S3 to store the database-to-archive mapping of each snapshot
**** Offer user option to use "expedited" retrievals so they go faster
*** TODO Support Amazon Cloud Drive
*** TODO Support Microsoft Azure blob storage
*** TODO Support Backblaze B2
*** TODO Support [[https://wiki.openstack.org/wiki/Swift][OpenStack Swift]]
*** TODO Support Wasabi
*** TODO Support Google Drive
*** TODO Support Google Cloud Coldline
*** TODO Support Dropbox
*** TODO Support Oracle Cloud Storage
*** TODO Support IBM Cloud Storage
*** TODO Support Rackspace Cloud Files
*** TODO Consider how to backup and restore FIFO, BLK, and CHR "files"
**** c.f. https://github.com/jborg/attic/blob/master/attic/archive.py
**** c.f. https://github.com/avz/node-mkfifo (for FIFO)
**** c.f. https://github.com/mafintosh/mknod (for BLK and CHR)
* Product
** TODO Define an MVP and work toward release
** TODO Evaluate other backup software
*** TODO Check out some on App Store
**** Backup Guru LE
**** ChronoSync Express
**** Backup
**** Remote Backup Magic
**** Sync - Backup and Restore
**** Backup for Dropbox
**** Freeze - for Amazon Glacier
*** Lot of "folder sync" apps out there
** TODO Define the target audience
*** Average home user, no technical expertise required
** TODO Need distinquishing features
*** What sets this application apart from the other polished products?
**** Cross-platform (e.g. macOS, Windows)
**** Linux server ready
** Application Monitoring
*** Need something to capture failures for debugging
**** c.f. https://sentry.io/welcome/
** Windows Certified
*** CloudBerry(?) has bunches of certifications
*** is that really so meaningful? *I* never cared
** Alternatives
*** Commercial
**** Arq
***** https://www.arqbackup.com
***** Windows, Mac
***** Uses a single master password
***** Supports numerous backends
**** Carbonite
***** https://www.carbonite.com
***** Consumer and business
***** Billed monthly
***** 128-bit encryption for all but most expensive plan
***** Windows, Mac, and "servers"
***** Seems to backup to their servers
**** CloudBerry
***** https://www.cloudberrylab.com/backup
***** Consumer and business
***** Windows, Mac, Linux
***** Supports Glacier and other services
***** Freeware version lacks compression, encryption, limited to 200GB
**** Duplicacy
***** https://github.com/gilbertchen/duplicacy
***** Lists other open source tools and compares them
***** Deduplicates chunks across systems
***** Does not use a database supposedly
***** Does not and can not support Glacier
**** JungleDisk
***** https://www.jungledisk.com/encrypted-backups/
***** Primarily business oriented
***** Seems to rely on their servers
***** Probably stores data elsewhere
**** Rebel Backup
***** https://www.svsware.com/rebelbackup
***** Encrypted backups to Dropbox or Google Drive
***** macOS only
**** qBackup
***** https://www.qualeed.com/en/qbackup/
***** Windows, Mac, Linux
***** Supports numerous backends
***** Has copious documentation with screen shots
**** tarsnap
***** https://www.tarsnap.com
***** Free client
***** Uses public key encryption rather than a password
***** Stores data in Amazon S3
***** Relies on tarsnap servers
***** 10x the price of Google Cloud or Amazon Glacier
***** Command-line interface
*** Open Source
**** Attic
***** https://attic-backup.org
***** Development stopped in 2015
***** Only supports SSH remote host
***** Command-line interface
**** Borg
***** https://borgbackup.readthedocs.io/en/stable/
***** Fork of Attic
***** Only supports SSH remote host
***** Command-line interface
**** bup
***** https://bup.github.io
***** Git-like (uses Python and Git) pack file storage
***** Requires a bup server for remote storage
***** Command-line interface
**** Duplicati
***** https://www.duplicati.com/
***** Requires .NET or Mono
***** Web-based interface
***** Windows and Linux
***** No Glacier support
**** duplicity
***** http://duplicity.nongnu.org
***** Uses GnuPG, a tar-like format, and rsync
***** Supports backends with a filesystem-like interface
***** Command-line interface
***** No Glacier support
**** rclone
***** https://github.com/rclone/rclone
***** Syncs a directory structure to the cloud
***** Offers chunking and encryption
***** Command-line interface
***** No Glacier support
**** restic
***** https://restic.net
***** Git-like data model
***** Supports numerous backends (no Glacier)
***** Command-line interface
** Name
*** Joseph suggests "Attic"
**** =atticapp.com= is taken
**** =attic.app= is for sale
**** Look for ~attic~ in different languages
**** Esperanto: ~mansardo~
***** also means something in Macedonian
**** Hawaiian: ~kaukau~
**** Latin: ~atticae~
* Documentation
** Duplicati has a fun description of how the backup works
*** files are broken into "bricks" which go in "bags" and stored in big "boxes" (the pack store)
*** c.f. https://duplicati.readthedocs.io/en/latest/01-introduction/
** TODO Third party license attributions
*** Include any/all third party license attribution somewhere
*** =cargo lichking bundle= will dump everything to the console
** TODO document how the user might change the passphrase over time
*** user must remember their old passwords in order to decrypt old pack files
*** the application will never store the actual password anywhere
*** will need to prompt the user when a different passphrase is needed
* Technical Information
** Data Growth
*** Database backup tgz seems to grow 8mb in 6 months
** Possible corner cases
*** Database backup, then restore, then pack prune
Because the database snapshot is recorded in the database after the snapshot
has already been uploaded, if the user were to restore the database and then
perform a pack pruning, the most recent database snapshot would be removed.
** JS Build Artifacts
*** Flutter => main.dart.js
| State      |    Size |
|------------+---------|
| production | 1742125 |
*** ReasonML + Webpack => main.js
| State       |    Size |
|-------------+---------|
| development | 2761882 |
| production  |  536345 |
| gzipped     |  145785 |
** Exploring other languages
*** Compile to native for easy deployment
*** Compile to native for code obfuscation
*** Rust
**** Advantages
***** compile to native
***** expressive, safe type system
***** good dependency management
***** lots of useful tools (e.g. clippy)
**** Disadvantages
***** fewer libraries compared to Go
**** DONE GraphQL server
***** Make sure it can generate a schema.json
***** Should be able to parse schema definition (for docs)
***** https://github.com/graphql-rust/juniper (BSD)
****** supports entire GraphQL specification
****** does /not/ read GraphQL schema language
****** supports GraphiQL and Playground
****** is not the HTTP server, but integrates with them
****** uses macros for schema documentation
***** tutorial at [[http://alex.amiran.it/post/2018-08-16-rust-graphql-webserver-with-warp-juniper-and-mongodb.html][alex.amiran.it]] that uses warp web framework
***** old https://github.com/nrc/graphql (MIT/Apache)
**** DONE Web framework
***** our needs are simple, so a simple framework is best
***** Actix https://actix.rs (Apache 2.0)
****** works with stable Rust
****** powerful and easy to use
****** testing library
****** integrates with juniper
****** offers state management for web code
****** lot more actively used than warp
***** warp https://github.com/seanmonstar/warp (MIT)
****** works with stable Rust
****** powerful and easy to use
****** testing library
****** integrates with juniper
***** Rocket https://rocket.rs (Apache 2.0)
****** requires Rust nightly because of fancy macros
****** routing using macros
****** streams input and output
****** cookies
****** json
****** environment configuration
****** testing library
****** integrates with juniper
***** Gotham https://gotham.rs (MIT/Apache 2.0)
****** targets stable Rust
****** routing
****** middleware
****** sessions
****** cookies
****** templates
****** testing library
****** how to integrate with juniper is unknown
***** Iron http://ironframework.io (MIT)
****** crate has not been updated since 2017
****** everything is middleware that must be added in
****** integrates with juniper
***** Nickel http://nickel-org.github.io (Express.js like) (MIT)
****** pretty basic compared to Rocket
***** tower-web https://github.com/carllerche/tower-web (MIT)
****** competing with warp? hyper?
**** DONE Database
***** ideally want something well maintained, reliable
***** schema is pretty simple, could use key/value store
***** RocksDB https://github.com/rust-rocksdb/rust-rocksdb (Apache)
****** statically links everything, including compression support
***** SQLite https://github.com/jgallagher/rusqlite (MIT)
***** Rust wrapper to LevelDB https://github.com/skade/leveldb
***** LevelDB in Rust (active?) https://bitbucket.org/dermesser/leveldb-rs/overview
**** DONE dotenv
***** https://github.com/dotenv-rs/dotenv (MIT)
**** DONE Configuration
***** https://github.com/mehcode/config-rs (MIT/Apache)
**** DONE =getpwuid= and =getgrgid= support
***** libc: https://crates.io/crates/libc (MIT/Apache 2.0)
**** DONE test library
***** https://github.com/rust-rspec/rspec (MPL-2.0)
****** appears to be dead
***** https://github.com/utkarshkukreti/speculate.rs (MIT)
****** works well for integration tests
**** DONE UUID support
***** https://github.com/uuid-rs/uuid (MIT/Apache 2.0)
**** DONE xattr support
***** Unix only: https://github.com/Stebalien/xattr (MIT/Apache 2.0)
**** DONE CDC
***** https://github.com/jrobhoward/quickcdc (MIT/Apache 2.0)
****** not quite FastCDC, given dates of paper, but should be close enough
****** use a constant salt value for predictable results
****** example uses =memmap= crate to read large files
**** DONE Tar file
***** https://github.com/alexcrichton/tar-rs (MIT/Apache 2.0)
**** DONE PGP/Encryption
***** https://github.com/gpg-rs/gpgme (LGPL)
****** will need to bundle the =gpgme= library (unless statically linked)
***** cryptostream https://github.com/neosmart/cryptostream (MIT)
***** basic packets [[https://github.com/csssuf/pretty-good][csssuf/pretty-good]]
***** read only [[https://nest.pijul.com/pmeunier/openpgp][pijul]] openpgp
**** DONE ULID
***** https://crates.io/crates/rusty_ulid (MIT)
**** DONE SFTP client
***** https://github.com/alexcrichton/ssh2-rs (MIT/Apache 2.0)
**** DONE AWS client
***** Rusoto https://www.rusoto.org (MIT)
**** DONE Google Cloud client
***** https://github.com/Byron/google-apis-rs (MIT/Apache 2.0)
**** DONE Minio client
***** Rusoto supports Minio https://github.com/rusoto/rusoto (MIT)
*** Go vs Rust
**** Go: first class support for cloud services
**** Go: statically linked OpenPGP readily available
**** Go: easy to read and write language
**** Rust: mature dependency management tooling
**** Rust: cargo has good editor support
**** Rust: expressive type system
**** Rust: nominal subtyping is much easier to follow
**** Rust: streamlined error handling
**** Rust: fine-grained namespaces and visibility control
